{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7678c661",
   "metadata": {
    "id": "7678c661"
   },
   "source": [
    "## 1. Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996c890a",
   "metadata": {
    "id": "996c890a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "import scipy.optimize as opt\n",
    "import scipy.linalg as sla\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309191eb",
   "metadata": {
    "id": "309191eb"
   },
   "source": [
    "## 2.Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd07528c",
   "metadata": {
    "id": "dd07528c"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    Xtr: np.ndarray  # noisy train+val\n",
    "    Str: np.ndarray  # noisy train+val\n",
    "    Xts: np.ndarray  # clean test\n",
    "    Yts: np.ndarray  # clean test\n",
    "\n",
    "def load_npz(path):\n",
    "    d=np.load(path);\n",
    "    return Dataset(d['Xtr'],d['Str'],d['Xts'],d['Yts'])\n",
    "\n",
    "#Other pipeline funcion define\n",
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def one_hot(y: np.ndarray, C: int) -> np.ndarray:\n",
    "    oh = np.zeros((y.shape[0], C), dtype=np.float32)\n",
    "    oh[np.arange(y.shape[0]), y] = 1.0\n",
    "    return oh\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    # stablize the value\n",
    "    z = z - z.max(axis=1, keepdims=True)\n",
    "    expz = np.exp(z)\n",
    "    return expz / (expz.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def accuracy(y, yhat): return float((y==yhat).mean())\n",
    "\n",
    "\n",
    "#retun T\n",
    "def get_T(name: str) -> np.ndarray:\n",
    "    name = name.lower()\n",
    "    if \"0.3\" in name:\n",
    "        T = np.array([[0.7, 0.3, 0.0],\n",
    "                      [0.0, 0.7, 0.3],\n",
    "                      [0.3, 0.0, 0.7]], dtype=np.float32)\n",
    "    elif \"0.6\" in name:\n",
    "        T = np.array([[0.4, 0.3, 0.3],\n",
    "                      [0.3, 0.4, 0.3],\n",
    "                      [0.3, 0.3, 0.4]], dtype=np.float32)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset: only 0.3 and 0.6 are supported here.\")\n",
    "    return T\n",
    "\n",
    "# split the training set\n",
    "def split(X,y,ratio=0.2,seed=42):\n",
    "    set_seed(seed);\n",
    "    n=X.shape[0];\n",
    "    idx=np.random.permutation(n);\n",
    "    sp=int(n*(1-ratio))\n",
    "    return X[idx[:sp]],y[idx[:sp]],X[idx[sp:]],y[idx[sp:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611ddf3",
   "metadata": {
    "id": "2611ddf3"
   },
   "source": [
    "## 3.Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29a327",
   "metadata": {
    "id": "ec29a327"
   },
   "source": [
    "### 3.1 Model Prepocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecb9e58",
   "metadata": {
    "id": "4ecb9e58"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Standardizer:\n",
    "    mean: np.ndarray; std: np.ndarray\n",
    "    def transform(self,Xf):\n",
    "        Xn=(Xf-self.mean)/self.std\n",
    "        bias=np.ones((Xn.shape[0],1),dtype=np.float64)\n",
    "        return np.hstack([Xn,bias])\n",
    "\n",
    "def flatten(X):\n",
    "    return X.reshape(X.shape[0],-1).astype(np.float64)\n",
    "def fit_std(Xtrf):\n",
    "    m=Xtrf.mean(0,keepdims=True);\n",
    "    s=Xtrf.std(0,keepdims=True)+1e-5;\n",
    "    return Standardizer(m,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62631634",
   "metadata": {
    "id": "62631634"
   },
   "source": [
    "### 3.2 Forward part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33141501",
   "metadata": {
    "id": "33141501"
   },
   "source": [
    "**Forward Method:**  \n",
    "Multiply the *clean prediction distribution* \\( p_{\\text{clean}} \\) by \\( T^\\top \\),  \n",
    "to obtain the predicted distribution of noisy labels:  \n",
    "$$\n",
    "p_{\\tilde{y}} = T^\\top \\, p_{\\text{clean}}\n",
    "$$\n",
    "Then compute the cross-entropy loss with respect to the noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a27e692",
   "metadata": {
    "id": "8a27e692"
   },
   "outputs": [],
   "source": [
    "def forward_loss(p_clean, y, T):\n",
    "    p_tilde = p_clean @ T\n",
    "    p_tilde = np.clip(p_tilde, 1e-12, 1.0)\n",
    "    return float(-np.log(p_tilde[np.arange(y.shape[0]), y]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225290ef",
   "metadata": {
    "id": "225290ef"
   },
   "source": [
    "Return $ \\frac{\\partial L}{\\partial z} $ (the gradient with respect to the logits),  \n",
    "which can be combined with the linear layer to compute  \n",
    "$ \\frac{\\partial L}{\\partial W} = X^\\top \\frac{\\partial L}{\\partial z} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e061503b",
   "metadata": {
    "id": "e061503b"
   },
   "outputs": [],
   "source": [
    "def dLdz_forward(p_clean, y, T):\n",
    "    N, C = p_clean.shape\n",
    "    Y = one_hot(y, C)\n",
    "    p_tilde = p_clean @ T\n",
    "    p_tilde = np.clip(p_tilde, 1e-12, 1.0)\n",
    "    dL_dp_tilde = -(Y / p_tilde) / N\n",
    "    dL_dp_clean = dL_dp_tilde @ T.T\n",
    "    s = (dL_dp_clean * p_clean).sum(axis=1, keepdims=True)\n",
    "    dL_dz = p_clean * (dL_dp_clean - s)\n",
    "    return dL_dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b148e9",
   "metadata": {
    "id": "87b148e9"
   },
   "source": [
    "### 3.3 Multiclass Logistic Regression (Softmax Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf1e84-5ae5-4127-9333-e0e62ec50dcf",
   "metadata": {
    "id": "dcaf1e84-5ae5-4127-9333-e0e62ec50dcf"
   },
   "source": [
    "This part applies a **softmax** function on the linear outputs (logits) to produce  \n",
    "a probability distribution over all classes:\n",
    "\n",
    "$ p(y = c \\mid x) = \\dfrac{\\exp(z_c)}{\\sum_{k=1}^{C} \\exp(z_k)} $,  \n",
    "where $ z = XW + b $ are the logits.\n",
    "\n",
    "The model is trained by minimizing the **cross-entropy loss** between the predicted  \n",
    "distribution $p(y \\mid x)$ and the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee93966",
   "metadata": {
    "id": "bee93966"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FwdCfg: wd:float=1e-4; max_iter:int=300; seed:int=42\n",
    "class SoftmaxFwd:\n",
    "    def __init__(self, D, C, T, cfg:FwdCfg):\n",
    "        self.D, self.C, self.T, self.cfg = D, C, T, cfg\n",
    "        set_seed(cfg.seed)\n",
    "        self.W = (0.01*np.random.randn(D,C)).astype(np.float64)\n",
    "    def _fun(self,w,X,y):\n",
    "        W=w.reshape(self.D,self.C); p=softmax(X@W)\n",
    "        base=forward_loss(p,y,self.T)\n",
    "        reg =0.5*self.cfg.wd*(sla.norm(W,'fro')**2)\n",
    "        dLdz=dLdz_forward(p,y,self.T)\n",
    "        grad = X.T@dLdz + self.cfg.wd*W\n",
    "        return base+reg, grad.reshape(-1)\n",
    "    def fit(self,X,y):\n",
    "        res=opt.minimize(self._fun,self.W.reshape(-1),args=(X,y),method=\"L-BFGS-B\",jac=True,\n",
    "                         options={\"maxiter\":self.cfg.max_iter})\n",
    "        self.W=res.x.reshape(self.D,self.C); return res\n",
    "    def predict_proba(self,X): return softmax(X@self.W)\n",
    "    def predict(self,X): return self.predict_proba(X).argmax(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ef5ab",
   "metadata": {
    "id": "146ef5ab"
   },
   "source": [
    "## 4. Hyper-parameter Tunning-Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47c3160",
   "metadata": {
    "id": "a47c3160"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Result:\n",
    "    cfg: \"FwdCfg\"\n",
    "    train_loss_mean: float\n",
    "    train_loss_std: float\n",
    "    val_loss_mean: float\n",
    "    val_loss_std: float\n",
    "    train_acc_mean: float\n",
    "    train_acc_std: float\n",
    "    val_acc_mean: float\n",
    "    val_acc_std: float\n",
    "\n",
    "#10 fold cross-validation for model selection\n",
    "def search_with_direction_cv(Xtr, Str, Xts, Yts, T, seed=42, n_splits=10) -> Tuple[Result, List[Result]]:\n",
    "    # Flatten\n",
    "    Xtr_f = flatten(Xtr)\n",
    "\n",
    "    C = T.shape[0]\n",
    "    D = Xtr_f.shape[1]\n",
    "\n",
    "    # Grid search\n",
    "    grid = [\n",
    "        FwdCfg(wd=1e-4, max_iter=500, seed=seed),\n",
    "        FwdCfg(wd=1e-3, max_iter=500, seed=seed),\n",
    "        FwdCfg(wd=1e-2, max_iter=500, seed=seed),\n",
    "        FwdCfg(wd=5e-2, max_iter=500, seed=seed),\n",
    "        FwdCfg(wd=1e-1, max_iter=500, seed=seed),\n",
    "        FwdCfg(wd=5e-1, max_iter=500, seed=seed),\n",
    "    ]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    # [(cfg, tr_mean, tr_std, va_mean, va_std, tr_acc_mean, tr_acc_std, va_acc_mean, va_acc_std)]\n",
    "    cfg_cv_stats = []\n",
    "\n",
    "    for cfg in grid:\n",
    "        fold_train_losses = []\n",
    "        fold_val_losses = []\n",
    "        fold_train_accs = []\n",
    "        fold_val_accs = []\n",
    "\n",
    "        for tr_idx, val_idx in skf.split(Xtr_f, Str):\n",
    "            X_tr_raw, y_tr = Xtr_f[tr_idx], Str[tr_idx]\n",
    "            X_val_raw, y_val = Xtr_f[val_idx], Str[val_idx]\n",
    "\n",
    "            # standlization\n",
    "            std = fit_std(X_tr_raw)\n",
    "            X_tr = std.transform(X_tr_raw)\n",
    "            X_val = std.transform(X_val_raw)\n",
    "\n",
    "            #train the model\n",
    "            model = SoftmaxFwd(X_tr.shape[1], C, T, cfg)\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "            #train loss & acc\n",
    "            p_tr = model.predict_proba(X_tr)\n",
    "            train_loss = forward_loss(p_tr, y_tr, T) + 0.5 * cfg.wd * (sla.norm(model.W, 'fro') ** 2)\n",
    "            y_tr_pred = np.argmax(p_tr, axis=1)\n",
    "            train_acc = accuracy(y_tr, y_tr_pred)\n",
    "\n",
    "            fold_train_losses.append(train_loss)\n",
    "            fold_train_accs.append(train_acc)\n",
    "\n",
    "            # val loss & acc\n",
    "            p_val = model.predict_proba(X_val)\n",
    "            val_loss = forward_loss(p_val, y_val, T) + 0.5 * cfg.wd * (sla.norm(model.W, 'fro') ** 2)\n",
    "            y_val_pred = np.argmax(p_val, axis=1)\n",
    "            val_acc = accuracy(y_val, y_val_pred)\n",
    "\n",
    "            fold_val_losses.append(val_loss)\n",
    "            fold_val_accs.append(val_acc)\n",
    "\n",
    "        # mean ± std\n",
    "        tr_mean = float(np.mean(fold_train_losses))\n",
    "        tr_std  = float(np.std(fold_train_losses))\n",
    "        va_mean = float(np.mean(fold_val_losses))\n",
    "        va_std  = float(np.std(fold_val_losses))\n",
    "\n",
    "        tr_acc_mean = float(np.mean(fold_train_accs))\n",
    "        tr_acc_std  = float(np.std(fold_train_accs))\n",
    "        va_acc_mean = float(np.mean(fold_val_accs))\n",
    "        va_acc_std  = float(np.std(fold_val_accs))\n",
    "\n",
    "        cfg_cv_stats.append(\n",
    "            (cfg, tr_mean, tr_std, va_mean, va_std, tr_acc_mean, tr_acc_std, va_acc_mean, va_acc_std)\n",
    "        )\n",
    "\n",
    "    # use val loss mean choose best cfg\n",
    "    best_tuple = min(cfg_cv_stats, key=lambda x: x[3])  # x[3] 是 val_loss_mean\n",
    "    best_cfg, best_tr_mean, best_tr_std, best_va_mean, best_va_std, \\\n",
    "        best_tr_acc_mean, best_tr_acc_std, best_va_acc_mean, best_va_acc_std = best_tuple\n",
    "\n",
    "    # results list\n",
    "    results: List[Result] = [\n",
    "        Result(\n",
    "            cfg=cfg,\n",
    "            train_loss_mean=tr_mean,\n",
    "            train_loss_std=tr_std,\n",
    "            val_loss_mean=va_mean,\n",
    "            val_loss_std=va_std,\n",
    "            train_acc_mean=tr_acc_mean,\n",
    "            train_acc_std=tr_acc_std,\n",
    "            val_acc_mean=va_acc_mean,\n",
    "            val_acc_std=va_acc_std,\n",
    "        )\n",
    "        for (cfg, tr_mean, tr_std, va_mean, va_std,\n",
    "             tr_acc_mean, tr_acc_std, va_acc_mean, va_acc_std) in cfg_cv_stats\n",
    "    ]\n",
    "\n",
    "    best = Result(\n",
    "        cfg=best_cfg,\n",
    "        train_loss_mean=best_tr_mean,\n",
    "        train_loss_std=best_tr_std,\n",
    "        val_loss_mean=best_va_mean,\n",
    "        val_loss_std=best_va_std,\n",
    "        train_acc_mean=best_tr_acc_mean,\n",
    "        train_acc_std=best_tr_acc_std,\n",
    "        val_acc_mean=best_va_acc_mean,\n",
    "        val_acc_std=best_va_acc_std,\n",
    "    )\n",
    "\n",
    "    return best, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739b0cb",
   "metadata": {
    "id": "4739b0cb"
   },
   "source": [
    "## 5. Model-selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4b8ab-117b-4413-b0e0-3a3914656847",
   "metadata": {
    "id": "50a4b8ab-117b-4413-b0e0-3a3914656847"
   },
   "source": [
    "### 5.1  Dataset with known flip rates (FashionMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58519e21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58519e21",
    "outputId": "29eccb5d-23e2-4182-855a-934e87fdf44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "\n",
      "==== Dataset: /content/drive/MyDrive/datasets/FashionMNIST0.3.npz ====\n",
      "Tuned configs with 10-fold CV (forward loss):\n",
      "  [p@T] wd=0.0001, it=500 | train_loss=0.6045 (±0.0008), val_loss=0.7443 (±0.0183), train_acc=72.40% (±0.19%), val_acc=67.64% (±0.54%)\n",
      "  [p@T] wd=0.001, it=500 | train_loss=0.6237 (±0.0006), val_loss=0.6781 (±0.0084), train_acc=70.89% (±0.09%), val_acc=68.24% (±0.69%)\n",
      "  [p@T] wd=0.01, it=500 | train_loss=0.6472 (±0.0007), val_loss=0.6646 (±0.0061), train_acc=69.34% (±0.08%), val_acc=68.72% (±0.70%)\n",
      "  [p@T] wd=0.05, it=500 | train_loss=0.6690 (±0.0007), val_loss=0.6761 (±0.0060), train_acc=68.83% (±0.08%), val_acc=68.54% (±0.67%)\n",
      "  [p@T] wd=0.1, it=500 | train_loss=0.6818 (±0.0007), val_loss=0.6865 (±0.0063), train_acc=68.61% (±0.09%), val_acc=68.45% (±0.69%)\n",
      "  [p@T] wd=0.5, it=500 | train_loss=0.7284 (±0.0007), val_loss=0.7301 (±0.0060), train_acc=68.11% (±0.08%), val_acc=68.06% (±0.61%)\n",
      "** Best: [p@T] wd=0.01, it=500 | train_loss=0.6472 (±0.0007), val_loss=0.6646 (±0.0061), train_acc=69.34% (±0.08%), val_acc=68.72% (±0.70%)\n",
      "\n",
      "==== Dataset: /content/drive/MyDrive/datasets/FashionMNIST0.6.npz ====\n",
      "Tuned configs with 10-fold CV (forward loss):\n",
      "  [p@T] wd=0.0001, it=500 | train_loss=1.0708 (±0.0004), val_loss=1.1026 (±0.0018), train_acc=51.47% (±0.26%), val_acc=36.74% (±0.76%)\n",
      "  [p@T] wd=0.001, it=500 | train_loss=1.0819 (±0.0001), val_loss=1.1002 (±0.0011), train_acc=47.11% (±0.17%), val_acc=37.12% (±1.01%)\n",
      "  [p@T] wd=0.01, it=500 | train_loss=1.0902 (±0.0001), val_loss=1.0955 (±0.0011), train_acc=40.44% (±0.13%), val_acc=38.29% (±0.95%)\n",
      "  [p@T] wd=0.05, it=500 | train_loss=1.0928 (±0.0001), val_loss=1.0945 (±0.0013), train_acc=39.21% (±0.08%), val_acc=38.77% (±0.85%)\n",
      "  [p@T] wd=0.1, it=500 | train_loss=1.0937 (±0.0001), val_loss=1.0948 (±0.0012), train_acc=39.05% (±0.09%), val_acc=38.72% (±0.69%)\n",
      "  [p@T] wd=0.5, it=500 | train_loss=1.0962 (±0.0001), val_loss=1.0966 (±0.0008), train_acc=38.75% (±0.12%), val_acc=38.60% (±0.55%)\n",
      "** Best: [p@T] wd=0.05, it=500 | train_loss=1.0928 (±0.0001), val_loss=1.0945 (±0.0013), train_acc=39.21% (±0.08%), val_acc=38.77% (±0.85%)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    set_seed(42)\n",
    "    datasets = [\n",
    "       \"datasets/FashionMNIST0.3.npz\",\n",
    "       \"datasets/FashionMNIST0.6.npz\",\n",
    "   ]\n",
    "\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    # datasets = [\n",
    "    #     \"/content/drive/MyDrive/datasets/FashionMNIST0.3.npz\",\n",
    "    #     \"/content/drive/MyDrive/datasets/FashionMNIST0.6.npz\",\n",
    "    # ]\n",
    "    for path in datasets:\n",
    "        print(f\"\\n==== Dataset: {path} ====\")\n",
    "        T = get_T(path)\n",
    "        data = load_npz(path)\n",
    "\n",
    "        best, results = search_with_direction_cv(\n",
    "            data.Xtr, data.Str, data.Xts, data.Yts, T, seed=42\n",
    "        )\n",
    "\n",
    "        def fmt(r: Result) -> str:\n",
    "            return (\n",
    "                f\"[p@T] wd={r.cfg.wd}, it={r.cfg.max_iter} | \"\n",
    "                f\"train_loss={r.train_loss_mean:.4f} (±{r.train_loss_std:.4f}), \"\n",
    "                f\"val_loss={r.val_loss_mean:.4f} (±{r.val_loss_std:.4f}), \"\n",
    "                f\"train_acc={r.train_acc_mean*100:.2f}% (±{r.train_acc_std*100:.2f}%), \"\n",
    "                f\"val_acc={r.val_acc_mean*100:.2f}% (±{r.val_acc_std*100:.2f}%)\"\n",
    "            )\n",
    "\n",
    "        print(\"Tuned configs with 10-fold CV (forward loss):\")\n",
    "        for r in results:\n",
    "            print(\" \", fmt(r))\n",
    "        print(\"** Best:\", fmt(best))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d76f1b-6d64-4f25-9978-09b9131a0c9d",
   "metadata": {
    "id": "24d76f1b-6d64-4f25-9978-09b9131a0c9d"
   },
   "source": [
    "### 5.2. Dataset with unknown flip rates (CIFAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de353bb-7383-427e-8ffc-396ad64a9f54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3de353bb-7383-427e-8ffc-396ad64a9f54",
    "outputId": "d65a8866-5e3d-4e22-f080-2ff63ed12d32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "\n",
      "==== Dataset: /content/drive/MyDrive/datasets/CIFAR.npz ====\n",
      "Tuned configs with 10-fold CV (forward loss):\n",
      "  [p@T] wd=0.0001, it=500 | train_loss=1.0777 (±0.0007), val_loss=1.0999 (±0.0020), train_acc=49.89% (±0.55%), val_acc=35.70% (±1.36%)\n",
      "  [p@T] wd=0.001, it=500 | train_loss=1.0838 (±0.0001), val_loss=1.1010 (±0.0016), train_acc=47.86% (±0.28%), val_acc=35.31% (±1.03%)\n",
      "  [p@T] wd=0.05, it=500 | train_loss=1.0938 (±0.0001), val_loss=1.0960 (±0.0012), train_acc=38.07% (±0.18%), val_acc=36.41% (±1.56%)\n",
      "  [p@T] wd=0.01, it=500 | train_loss=1.0912 (±0.0001), val_loss=1.0965 (±0.0012), train_acc=40.43% (±0.13%), val_acc=36.23% (±1.18%)\n",
      "  [p@T] wd=0.5, it=500 | train_loss=1.0966 (±0.0001), val_loss=1.0971 (±0.0008), train_acc=36.70% (±0.20%), val_acc=36.25% (±1.31%)\n",
      "  [p@T] wd=0.1, it=500 | train_loss=1.0948 (±0.0001), val_loss=1.0962 (±0.0011), train_acc=37.69% (±0.21%), val_acc=36.35% (±1.57%)\n",
      "  [p@T] wd=0.2, it=500 | train_loss=1.0956 (±0.0001), val_loss=1.0965 (±0.0010), train_acc=37.28% (±0.21%), val_acc=36.43% (±1.42%)\n",
      "  [p@T] wd=0.5, it=500 | train_loss=1.0966 (±0.0001), val_loss=1.0971 (±0.0008), train_acc=36.70% (±0.20%), val_acc=36.25% (±1.31%)\n",
      "** Best (by 10-fold mean val loss): [p@T] wd=0.05, it=500 | train_loss=1.0938 (±0.0001), val_loss=1.0960 (±0.0012), train_acc=38.07% (±0.18%), val_acc=36.41% (±1.56%)\n"
     ]
    }
   ],
   "source": [
    "# ==== CIFAR (ForwardCorrection, softmax) - 10-fold CV on wd (fixed dims) ====\n",
    "\n",
    "# T from Transition matrix estimator\n",
    "T_CIFAR = np.array([[0.3753,0.3311 ,0.2935],\n",
    "                    [0.3028,0.3493,0.3479],\n",
    "                    [0.3277,0.2998,0.3725]], dtype=np.float64)\n",
    "DATASET_PATH = \"datasets/CIFAR.npz\"\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# DATASET_PATH =\"/content/drive/MyDrive/datasets/CIFAR.npz\"\n",
    "\n",
    "# load data\n",
    "d = np.load(DATASET_PATH)\n",
    "Xtr, Str, Xts, Yts = d['Xtr'], d['Str'], d['Xts'], d['Yts']\n",
    "\n",
    "# Flattern\n",
    "Xtr_f = flatten(Xtr)\n",
    "Xts_f = flatten(Xts)\n",
    "\n",
    "@dataclass\n",
    "class FwdCfg:\n",
    "    wd: float = 0.05\n",
    "    max_iter: int = 500\n",
    "    seed: int = 42\n",
    "\n",
    "C = T_CIFAR.shape[0]\n",
    "seed = 42\n",
    "\n",
    "# Grid Search\n",
    "grid = [\n",
    "    FwdCfg(wd=1e-4, max_iter=500, seed=seed),\n",
    "    FwdCfg(wd=1e-3, max_iter=500, seed=seed),\n",
    "    FwdCfg(wd=5e-2, max_iter=500, seed=seed),\n",
    "    FwdCfg(wd=1e-2, max_iter=500, seed=seed),\n",
    "    FwdCfg(wd=5e-1, max_iter=500, seed=seed),\n",
    "    FwdCfg(wd=1e-1, max_iter=500, seed=seed),\n",
    "    FwdCfg(wd=0.2,   max_iter=500, seed=seed),\n",
    "    FwdCfg(wd=0.5,   max_iter=500, seed=seed),\n",
    "]\n",
    "\n",
    "print(f\"\\n==== Dataset: {DATASET_PATH} ====\")\n",
    "print(\"Tuned configs with 10-fold CV (forward loss):\")\n",
    "\n",
    "# 10 fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "# Model selection\n",
    "cv_summaries = []\n",
    "# [(cfg, mean_tr_loss, std_tr_loss, mean_vloss, std_vloss,\n",
    "#    mean_tr_acc, std_tr_acc, mean_vacc, std_vacc)]\n",
    "\n",
    "for cfg in grid:\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses   = []\n",
    "    fold_train_accs   = []\n",
    "    fold_val_accs     = []\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(Xtr_f, Str):\n",
    "\n",
    "        X_tr_raw, y_tr   = Xtr_f[tr_idx], Str[tr_idx]\n",
    "        X_val_raw, y_val = Xtr_f[val_idx], Str[val_idx]\n",
    "\n",
    "        std   = fit_std(X_tr_raw)\n",
    "        X_tr  = std.transform(X_tr_raw)\n",
    "        X_val = std.transform(X_val_raw)\n",
    "\n",
    "        D_fold = X_tr.shape[1]\n",
    "\n",
    "        # training model\n",
    "        model = SoftmaxFwd(D_fold, C, T_CIFAR, cfg)\n",
    "        res = model.fit(X_tr, y_tr)\n",
    "\n",
    "        # training metrics\n",
    "        p_tr = model.predict_proba(X_tr)\n",
    "        tr_loss = forward_loss(p_tr, y_tr, T_CIFAR) + 0.5 * cfg.wd * (sla.norm(model.W, 'fro')**2)\n",
    "        y_pred_tr = model.predict(X_tr)\n",
    "        tr_acc = (y_pred_tr == y_tr).mean()\n",
    "\n",
    "        # validation metrics\n",
    "        p_val = model.predict_proba(X_val)\n",
    "        vloss = forward_loss(p_val, y_val, T_CIFAR) + 0.5 * cfg.wd * (sla.norm(model.W, 'fro')**2)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        val_acc = (y_pred_val == y_val).mean()\n",
    "\n",
    "        fold_train_losses.append(float(tr_loss))\n",
    "        fold_val_losses.append(float(vloss))\n",
    "        fold_train_accs.append(float(tr_acc))\n",
    "        fold_val_accs.append(float(val_acc))\n",
    "\n",
    "    # each fold mean and std\n",
    "    mean_tr_loss = float(np.mean(fold_train_losses))\n",
    "    std_tr_loss  = float(np.std(fold_train_losses, ddof=1)) if len(fold_train_losses) > 1 else 0.0\n",
    "\n",
    "    mean_vloss   = float(np.mean(fold_val_losses))\n",
    "    std_vloss    = float(np.std(fold_val_losses, ddof=1)) if len(fold_val_losses) > 1 else 0.0\n",
    "\n",
    "    mean_tr_acc  = float(np.mean(fold_train_accs))\n",
    "    std_tr_acc   = float(np.std(fold_train_accs, ddof=1)) if len(fold_train_accs) > 1 else 0.0\n",
    "\n",
    "    mean_vacc    = float(np.mean(fold_val_accs))\n",
    "    std_vacc     = float(np.std(fold_val_accs, ddof=1)) if len(fold_val_accs) > 1 else 0.0\n",
    "\n",
    "    cv_summaries.append(\n",
    "        (cfg,\n",
    "         mean_tr_loss, std_tr_loss,\n",
    "         mean_vloss,   std_vloss,\n",
    "         mean_tr_acc,  std_tr_acc,\n",
    "         mean_vacc,    std_vacc)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"  [p@T] wd={cfg.wd:g}, it={cfg.max_iter} | \"\n",
    "        f\"train_loss={mean_tr_loss:.4f} (±{std_tr_loss:.4f}), \"\n",
    "        f\"val_loss={mean_vloss:.4f} (±{std_vloss:.4f}), \"\n",
    "        f\"train_acc={mean_tr_acc*100:.2f}% (±{std_tr_acc*100:.2f}%), \"\n",
    "        f\"val_acc={mean_vacc*100:.2f}% (±{std_vacc*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "# use mean validation loss choose best cfg ===\n",
    "\n",
    "(best_cfg,\n",
    " best_mean_trloss, best_std_trloss,\n",
    " best_mean_vloss,  best_std_vloss,\n",
    " best_mean_tracc,  best_std_tracc,\n",
    " best_mean_vacc,   best_std_vacc) = min(cv_summaries, key=lambda x: x[3])\n",
    "\n",
    "# Results Summary\n",
    "results = []\n",
    "for (cfg,\n",
    "     mean_trloss, std_trloss,\n",
    "     mean_vloss,  std_vloss,\n",
    "     mean_tracc,  std_tracc,\n",
    "     mean_vacc,   std_vacc) in cv_summaries:\n",
    "\n",
    "    results.append({\n",
    "              \"wd\": cfg.wd,\n",
    "              \"iters\": \"N/A\",\n",
    "              \"train_loss\": mean_trloss,\n",
    "              \"train_loss_std\": std_trloss,\n",
    "              \"val_loss\": mean_vloss,\n",
    "              \"val_loss_std\": std_vloss,\n",
    "              \"train_acc\": mean_tracc,\n",
    "              \"train_acc_std\": std_tracc,\n",
    "              \"val_acc\": mean_vacc,\n",
    "              \"val_acc_std\": std_vacc,\n",
    "})\n",
    "\n",
    "print(\n",
    "    f\"** Best (by 10-fold mean val loss): [p@T] wd={best_cfg.wd:g}, it={best_cfg.max_iter} | \"\n",
    "    f\"train_loss={best_mean_trloss:.4f} (±{best_std_trloss:.4f}), \"\n",
    "    f\"val_loss={best_mean_vloss:.4f} (±{best_std_vloss:.4f}), \"\n",
    "    f\"train_acc={best_mean_tracc*100:.2f}% (±{best_std_tracc*100:.2f}%), \"\n",
    "    f\"val_acc={best_mean_vacc*100:.2f}% (±{best_std_vacc*100:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecb488-d4fb-47e1-be86-96b92876dcf6",
   "metadata": {
    "id": "12ecb488-d4fb-47e1-be86-96b92876dcf6"
   },
   "source": [
    "## 6. Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a64c5d-0837-4524-8e49-2d05ea584e83",
   "metadata": {
    "id": "73a64c5d-0837-4524-8e49-2d05ea584e83"
   },
   "outputs": [],
   "source": [
    "def train_config_forward_once(\n",
    "    dataset_path: str,\n",
    "    cfg: FwdCfg,\n",
    "    tr_idx: np.ndarray = None,\n",
    "    va_idx: np.ndarray = None,\n",
    "    seed: int = 42,\n",
    "):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # load data\n",
    "    d = np.load(dataset_path)\n",
    "    Xtr, Str, Xts, Yts = d[\"Xtr\"], d[\"Str\"], d[\"Xts\"], d[\"Yts\"]\n",
    "\n",
    "    # import T\n",
    "    if \"FashionMNIST0.3\" in dataset_path:\n",
    "        T = np.array([\n",
    "            [0.7, 0.3, 0.0],\n",
    "            [0.0, 0.7, 0.3],\n",
    "            [0.3, 0.0, 0.7]\n",
    "        ], dtype=np.float32)\n",
    "    elif \"FashionMNIST0.6\" in dataset_path:\n",
    "        T = np.array([\n",
    "            [0.4, 0.3, 0.3],\n",
    "            [0.3, 0.4, 0.3],\n",
    "            [0.3, 0.3, 0.4]\n",
    "        ], dtype=np.float32)\n",
    "    elif \"CIFAR\" in dataset_path:\n",
    "        T = np.array([\n",
    "            [0.3753, 0.3311, 0.2935],\n",
    "            [0.3028, 0.3493, 0.3479],\n",
    "            [0.3277, 0.2998, 0.3725]\n",
    "        ], dtype=np.float64)\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized dataset_path: {dataset_path}\")\n",
    "\n",
    "    C = T.shape[0]\n",
    "\n",
    "    # split the data if no idx input\n",
    "    if tr_idx is None or va_idx is None:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        tr_idx, va_idx = train_test_split(\n",
    "            np.arange(len(Str)), test_size=0.1, stratify=Str, random_state=seed\n",
    "        )\n",
    "\n",
    "    # split the data base on idx\n",
    "    X_train, y_train = Xtr[tr_idx], Str[tr_idx]\n",
    "    X_val, y_val = Xtr[va_idx], Str[va_idx]\n",
    "\n",
    "    # ==== flatten & standardization ====\n",
    "    X_train_f = flatten(X_train)\n",
    "    X_val_f   = flatten(X_val)\n",
    "    X_test_f  = flatten(Xts)\n",
    "\n",
    "    std = fit_std(X_train_f)\n",
    "    X_train_std = std.transform(X_train_f)\n",
    "    X_val_std   = std.transform(X_val_f)\n",
    "    X_test_std  = std.transform(X_test_f)\n",
    "\n",
    "    D = X_train_std.shape[1]\n",
    "\n",
    "    # Training\n",
    "    model = SoftmaxFwd(D, C, T, cfg)\n",
    "    res = model.fit(X_train_std, y_train)\n",
    "\n",
    "    # Validation\n",
    "    p_val = model.predict_proba(X_val_std)\n",
    "    base_loss = forward_loss(p_val, y_val, T)\n",
    "    reg_loss  = 0.5 * cfg.wd * (sla.norm(model.W, 'fro') ** 2)\n",
    "    val_loss  = base_loss + reg_loss\n",
    "    val_acc   = accuracy(y_val, model.predict(X_val_std))\n",
    "\n",
    "    # testing\n",
    "    test_acc  = accuracy(Yts, model.predict(X_test_std))\n",
    "\n",
    "    return val_loss, val_acc, test_acc, model, std\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
