{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16df67f1-a16d-421d-a6a0-0f81fb0235be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MKL_THREADING_LAYER=SEQUENTIAL\n",
      "env: OMP_NUM_THREADS=1\n",
      "env: MKL_NUM_THREADS=1\n",
      "env: NUMEXPR_NUM_THREADS=1\n"
     ]
    }
   ],
   "source": [
    "#不运行这个kernel在运行softmax时会自动restart（不知道为啥\n",
    "%env MKL_THREADING_LAYER=SEQUENTIAL\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env MKL_NUM_THREADS=1\n",
    "%env NUMEXPR_NUM_THREADS=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4caed3-ab2e-48a8-a448-e77a4386bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import random\n",
    "import scipy.optimize as opt\n",
    "import scipy.linalg as sla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8a9bb1-c851-4b09-ba09-b4848394a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载（复用作业给的风格）\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    Xtr: np.ndarray  # noisy train+val 的特征\n",
    "    Str: np.ndarray  # noisy train+val 的标签（{0,1,2}）\n",
    "    Xts: np.ndarray  # clean test 的特征\n",
    "    Yts: np.ndarray  # clean test 的标签（{0,1,2}）\n",
    "\n",
    "def load_npz(path): \n",
    "    d=np.load(path); \n",
    "    return Dataset(d['Xtr'],d['Str'],d['Xts'],d['Yts'])\n",
    "\n",
    "#其他函数定义\n",
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def one_hot(y: np.ndarray, C: int) -> np.ndarray:\n",
    "    oh = np.zeros((y.shape[0], C), dtype=np.float32)\n",
    "    oh[np.arange(y.shape[0]), y] = 1.0\n",
    "    return oh\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    z = z - z.max(axis=1, keepdims=True)       # 数值稳定\n",
    "    expz = np.exp(z)\n",
    "    return expz / (expz.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return float((y_true == y_pred).mean())\n",
    "\n",
    "def split(X, y, ratio=0.2, seed=42):\n",
    "    set_seed(seed)\n",
    "    n = X.shape[0]\n",
    "    idx = np.random.permutation(n)\n",
    "    sp = int(n * (1 - ratio))\n",
    "    return X[idx[:sp]], y[idx[:sp]], X[idx[sp:]], y[idx[sp:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7271a2e-3a88-4f0b-ac63-29554aa7b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_train_val(Xtr, Xva):\n",
    "    Xtr = Xtr.astype(np.float32)\n",
    "    Xva = Xva.astype(np.float32)\n",
    "    if Xtr.max() > 1.5:\n",
    "        Xtr /= 255.0\n",
    "        Xva /= 255.0\n",
    "    mu = Xtr.mean(axis=0, keepdims=True)\n",
    "    sg = Xtr.std(axis=0, keepdims=True) + 1e-6\n",
    "    Xtr = (Xtr - mu) / sg\n",
    "    Xva = (Xva - mu) / sg\n",
    "    return Xtr, Xva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bae360-87b1-47da-bfb7-ee4abdbbd9f9",
   "metadata": {},
   "source": [
    "# 1 Warmup Model 初步分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79807d4f-49e6-4bc7-a1c1-a0f73b6b4b09",
   "metadata": {},
   "source": [
    "## 1.1 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3233ff51-5bac-4e73-b218-491dae326487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWarmup:\n",
    "    \"\"\"简单多类 softmax 回归 (NumPy版)\"\"\"\n",
    "    def __init__(self, in_dim: int, num_classes: int, lr: float = 0.05, weight_decay: float = 5e-4):\n",
    "        self.C = num_classes\n",
    "        self.W = 0.01 * np.random.randn(in_dim, num_classes).astype(np.float32)\n",
    "        self.b = np.zeros((num_classes,), dtype=np.float32)\n",
    "        self.lr = lr\n",
    "        self.wd = weight_decay\n",
    "\n",
    "    def _forward(self, X):\n",
    "        logits = X @ self.W + self.b\n",
    "        return softmax(logits)\n",
    "\n",
    "    def _loss_and_grads(self, X, y):\n",
    "        N = X.shape[0]\n",
    "        P = self._forward(X)\n",
    "        loss = -np.log(P[np.arange(N), y] + 1e-12).mean()\n",
    "        if self.wd > 0:\n",
    "            loss += 0.5 * self.wd * np.sum(self.W * self.W)\n",
    "        G = P\n",
    "        G[np.arange(N), y] -= 1.0\n",
    "        G /= N\n",
    "        dW = X.T @ G + self.wd * self.W\n",
    "        db = G.sum(axis=0)\n",
    "        return loss, dW, db\n",
    "\n",
    "    def fit(self, X, y, Xval=None, yval=None, epochs=10, batch_size=256, verbose=True, seed=0):\n",
    "        set_seed(seed)\n",
    "        N = X.shape[0]\n",
    "        for ep in range(1, epochs + 1):\n",
    "            idx = np.random.permutation(N)\n",
    "            Xs, ys = X[idx], y[idx]\n",
    "            for st in range(0, N, batch_size):\n",
    "                ed = min(st + batch_size, N)\n",
    "                loss, dW, db = self._loss_and_grads(Xs[st:ed], ys[st:ed])\n",
    "                self.W -= self.lr * dW\n",
    "                self.b -= self.lr * db\n",
    "            # 每个 epoch 打印一次训练和验证准确率\n",
    "            if verbose:\n",
    "                tr_acc = self.score(X, y)\n",
    "                msg = f\"[Softmax Warmup] epoch {ep:02d} | train_acc={tr_acc:.3f}\"\n",
    "                if Xval is not None:\n",
    "                    va_acc = self.score(Xval, yval)\n",
    "                    msg += f\", val_acc={va_acc:.3f}\"\n",
    "                print(msg)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self._forward(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self._forward(X), axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return accuracy(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21faa7b3-4ea7-4912-84e9-bc640ecb0991",
   "metadata": {},
   "source": [
    "## 1.2 CNN（没有用，可删）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9158a2da-9237-4db6-b959-ad8780b228e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== CNN =====\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# def _to_float01(x: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"把数据转 float32，并在需要时从 0-255 归一到 0-1。\"\"\"\n",
    "#     x = x.astype(np.float32)\n",
    "#     if x.max() > 1.5:\n",
    "#         x = x / 255.0\n",
    "#     return x\n",
    "\n",
    "# def _reshape_to_nchw(X: np.ndarray, image_shape=(28, 28), channels=1) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     把 (N, D) 或 (N, H*W*C) 的扁平数组重塑成 (N, C, H, W)。\n",
    "#     image_shape: (H, W); channels: 1(灰度) 或 3(RGB)\n",
    "#     \"\"\"\n",
    "#     N = X.shape[0]\n",
    "#     H, W = image_shape\n",
    "#     if X.ndim == 2:\n",
    "#         X = X.reshape(N, H, W, channels)  # NHWC\n",
    "#     # NHWC -> NCHW\n",
    "#     if X.shape[-1] == channels:\n",
    "#         X = np.transpose(X, (0, 3, 1, 2))\n",
    "#     return X\n",
    "\n",
    "# def _compute_channel_stats(X_nchw: np.ndarray):\n",
    "#     \"\"\"按通道计算训练集均值/方差（用于标准化）\"\"\"\n",
    "#     # X: (N, C, H, W)\n",
    "#     mu = X_nchw.mean(axis=(0, 2, 3), keepdims=True)\n",
    "#     sg = X_nchw.std(axis=(0, 2, 3), keepdims=True) + 1e-6\n",
    "#     return mu, sg\n",
    "\n",
    "# class _SimpleCNN(nn.Module):\n",
    "#     \"\"\"\n",
    "#     极简 CNN：\n",
    "#       Conv( C->32 ) -> ReLU -> MaxPool\n",
    "#       Conv(32->64 ) -> ReLU -> MaxPool\n",
    "#       Flatten -> FC -> Softmax(通过CE)\n",
    "#     适合 28x28 或 32x32 小图。\n",
    "#     \"\"\"\n",
    "#     def __init__(self, in_ch: int, num_classes: int, img_hw: int):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_ch, 32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.pool  = nn.MaxPool2d(2, 2)\n",
    "#         # 下采样两次：H,W 各 /4\n",
    "#         feat_hw = img_hw // 4\n",
    "#         self.fc   = nn.Linear(64 * feat_hw * feat_hw, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))  # (N,32,H/2,W/2)\n",
    "#         x = self.pool(F.relu(self.conv2(x)))  # (N,64,H/4,W/4)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc(x)\n",
    "#         return x  # logits\n",
    "\n",
    "# class CNNWarmup:\n",
    "#     \"\"\"\n",
    "#     用小型 CNN 做 warm-up（拟合 noisy 标签），暴露 fit / predict_proba / predict / score。\n",
    "#     会：\n",
    "#       - 自动把 numpy 扁平数组重塑为 (N,C,H,W)\n",
    "#       - 用训练集通道均值/方差做标准化（验证集复用同统计量）\n",
    "#     \"\"\"\n",
    "#     def __init__(self, image_shape=(28, 28), channels=1, num_classes=3,\n",
    "#                  lr=1e-3, weight_decay=5e-4, batch_size=256, epochs=10, seed=0, device=None):\n",
    "#         self.H, self.W = image_shape\n",
    "#         self.C = channels\n",
    "#         self.K = num_classes\n",
    "#         self.lr = lr\n",
    "#         self.wd = weight_decay\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.seed = seed\n",
    "#         self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         torch.manual_seed(seed)\n",
    "#         np.random.seed(seed)\n",
    "\n",
    "#         self.model = _SimpleCNN(self.C, self.K, img_hw=self.H).to(self.device)\n",
    "#         self.mu = None\n",
    "#         self.sg = None  # 训练集通道均值/方差（NCHW 维度下 C×1×1）\n",
    "#         self.tr_acc = None\n",
    "#         self.va_acc = None\n",
    "\n",
    "#     def _prep_xy(self, X: np.ndarray, y: np.ndarray, is_train=False):\n",
    "#         \"\"\"把 numpy X,y 转成标准化后的 torch Tensor\"\"\"\n",
    "#         X = _to_float01(X)\n",
    "#         X = _reshape_to_nchw(X, (self.H, self.W), self.C)\n",
    "#         if is_train:\n",
    "#             self.mu, self.sg = _compute_channel_stats(X)\n",
    "#         assert self.mu is not None and self.sg is not None, \"Must fit on train before val.\"\n",
    "#         X = (X - self.mu) / self.sg\n",
    "#         X = torch.from_numpy(X).float()\n",
    "#         y = torch.from_numpy(y.astype(np.int64))\n",
    "#         return X, y\n",
    "\n",
    "#     def fit(self, Xtr: np.ndarray, ytr: np.ndarray, Xva: np.ndarray=None, yva: np.ndarray=None, verbose=True):\n",
    "#         \"\"\"训练 CNN 并在每个 epoch 打印 train/val acc。\"\"\"\n",
    "#         # 预处理 & DataLoader\n",
    "#         Xtr_t, ytr_t = self._prep_xy(Xtr, ytr, is_train=True)\n",
    "#         tr_loader = DataLoader(TensorDataset(Xtr_t, ytr_t), batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "#         if Xva is not None:\n",
    "#             Xva_t, yva_t = self._prep_xy(Xva, yva, is_train=False)\n",
    "#             va_loader = DataLoader(TensorDataset(Xva_t, yva_t), batch_size=self.batch_size, shuffle=False)\n",
    "#         else:\n",
    "#             va_loader = None\n",
    "\n",
    "#         opt = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "#         for ep in range(1, self.epochs + 1):\n",
    "#             self.model.train()\n",
    "#             for xb, yb in tr_loader:\n",
    "#                 xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "#                 opt.zero_grad()\n",
    "#                 logits = self.model(xb)\n",
    "#                 loss = F.cross_entropy(logits, yb)\n",
    "#                 loss.backward()\n",
    "#                 opt.step()\n",
    "\n",
    "#             # ---- 监控：train / val acc ----\n",
    "#             self.model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 tr_acc = self._accuracy_loader(tr_loader)\n",
    "#                 msg = f\"[CNN Warmup] epoch {ep:02d} | train_acc={tr_acc:.3f}\"\n",
    "#                 if va_loader is not None:\n",
    "#                     va_acc = self._accuracy_loader(va_loader)\n",
    "#                     msg += f\", val_acc={va_acc:.3f}\"\n",
    "#                     self.va_acc = va_acc\n",
    "#                 self.tr_acc = tr_acc\n",
    "#                 if verbose: print(msg)\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def _accuracy_loader(self, loader):\n",
    "#         total = correct = 0\n",
    "#         for xb, yb in loader:\n",
    "#             xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "#             logits = self.model(xb)\n",
    "#             pred = logits.argmax(1)\n",
    "#             correct += (pred == yb).sum().item()\n",
    "#             total += yb.numel()\n",
    "#         return correct / total\n",
    "\n",
    "#     # === 下面三个方法保持和 SoftmaxWarmup 同名，以便无缝替换 ===\n",
    "#     @torch.no_grad()\n",
    "#     def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "#         \"\"\"返回 softmax 概率 (N, K)\"\"\"\n",
    "#         self.model.eval()\n",
    "#         X = _to_float01(X)\n",
    "#         X = _reshape_to_nchw(X, (self.H, self.W), self.C)\n",
    "#         X = (X - self.mu) / self.sg\n",
    "#         X_t = torch.from_numpy(X).float().to(self.device)\n",
    "#         probs = F.softmax(self.model(X_t), dim=1).cpu().numpy()\n",
    "#         return probs\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "#         return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "#     def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "#         yhat = self.predict(X)\n",
    "#         return float((yhat == y).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb443e22-7ecd-4212-990a-ff7e4410f610",
   "metadata": {},
   "source": [
    "# 2 T估计器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd95d5-aeeb-445c-9daf-9902614af7df",
   "metadata": {},
   "source": [
    "## 2.1 选前百分之多少的样本作为锚点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ae079a-1624-4078-aa5d-4be34d0c8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_anchors_topk(probs: np.ndarray, top_pct: float = 0.01):\n",
    "    N, C = probs.shape\n",
    "    k = max(1, int(np.ceil(top_pct * N)))\n",
    "    anchors = []\n",
    "    for i in range(C):\n",
    "        idx = np.argsort(probs[:, i])[-k:]  # 概率最高的 k 个\n",
    "        anchors.append(idx)\n",
    "    return anchors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0650e5e3-df5b-441b-9df2-3aadbac22084",
   "metadata": {},
   "source": [
    "## 2.2 估计 T 的每一行（两种常见法：概率平均 / 计数直方图）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d78ebd1-6173-4e96-adc2-8a4c1d76881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_T_anchor(probs: np.ndarray,\n",
    "                      noisy_labels: np.ndarray,\n",
    "                      anchors: list,\n",
    "                      use_counts: bool = False,\n",
    "                      eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    用锚点估计转移矩阵 T。\n",
    "    - 概率平均法（use_counts=False）：对锚点样本的概率向量取均值；\n",
    "    - 计数法（use_counts=True）：对锚点样本的 noisy 标签计频率。\n",
    "    \"\"\"\n",
    "    N, C = probs.shape\n",
    "    T_hat = np.zeros((C, C), dtype=np.float32)\n",
    "    for i in range(C):\n",
    "        idx = anchors[i]\n",
    "        if len(idx) == 0:\n",
    "            row = np.zeros(C, dtype=np.float32); row[i] = 1.0\n",
    "        else:\n",
    "            if use_counts:\n",
    "                hist = np.bincount(noisy_labels[idx], minlength=C).astype(np.float32)\n",
    "                row = hist / (hist.sum() + eps)\n",
    "            else:\n",
    "                row = probs[idx].mean(axis=0).astype(np.float32)\n",
    "                row = row / (row.sum() + eps)\n",
    "        row = np.clip(row, eps, 1.0)\n",
    "        row = row / row.sum()\n",
    "        T_hat[i] = row\n",
    "    return T_hat\n",
    "\n",
    "def estimate_T_from_probs(probs: np.ndarray,\n",
    "                          noisy_labels: np.ndarray,\n",
    "                          top_pct: float = 0.01,\n",
    "                          use_counts: bool = False,\n",
    "                          verbose: bool = True):\n",
    "    \"\"\"\n",
    "    入口：直接从 P= predict_proba(X) 开始估计 T。\n",
    "    - probs: (N,C)  warm-up 输出的 softmax 概率（和 noisy_labels 一一对应）\n",
    "    - noisy_labels: (N,)\n",
    "    \"\"\"\n",
    "    assert probs.shape[0] == noisy_labels.shape[0], \"probs 与 noisy_labels 数量必须一致。\"\n",
    "    C = probs.shape[1]\n",
    "\n",
    "    # 1) 选锚点\n",
    "    anchors = select_anchors_topk(probs, top_pct=top_pct)\n",
    "    if verbose:\n",
    "        cnts = [len(a) for a in anchors]\n",
    "        print(f\"[Anchor] top_pct={top_pct:.4f}, per-class counts={cnts}\")\n",
    "\n",
    "    # 2) 估计 T\n",
    "    T_hat = estimate_T_anchor(probs, noisy_labels, anchors, use_counts=use_counts)\n",
    "\n",
    "    # 3) 简单检查与打印\n",
    "    if verbose:\n",
    "        row_sums = T_hat.sum(axis=1)\n",
    "        print(\"[T_hat] row sums:\", np.round(row_sums, 6))\n",
    "        # 按需：整数去掉小数点的友好打印\n",
    "        def smart_round(x):\n",
    "            s = f\"{x:.4f}\".rstrip('0').rstrip('.')\n",
    "            return s if s != '' else '0'\n",
    "        print(\"T_hat:\\n\",\n",
    "              np.array2string(T_hat, formatter={'float_kind': smart_round}, separator=' '))\n",
    "\n",
    "    return T_hat, anchors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3451e50-5c0c-4e4c-929f-a054f0eba7f5",
   "metadata": {},
   "source": [
    "# 3 看初步分类器在数据集上的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4551eb-39ee-4168-8032-bcd2e489d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = load_npz(\"datasets/FashionMNIST0.3.npz\")\n",
    "data2 = load_npz(\"datasets/FashionMNIST0.6.npz\")\n",
    "data3 = load_npz(\"datasets/CIFAR.npz\")\n",
    "\n",
    "Xtr_1, ytr_1, Xva_1, yva_1 = split(data1.Xtr, data1.Str, ratio=0.2, seed=0)\n",
    "Xtr_1_std, Xva_1_std = standardize_train_val(Xtr_1, Xva_1)\n",
    "\n",
    "Xtr_2, ytr_2, Xva_2, yva_2 = split(data2.Xtr, data2.Str, ratio=0.2, seed=0)\n",
    "Xtr_2_std, Xva_2_std = standardize_train_val(Xtr_2, Xva_2)\n",
    "\n",
    "Xtr_3, ytr_3, Xva_3, yva_3 = split(data3.Xtr, data3.Str, ratio=0.2, seed=0)\n",
    "#flatten\n",
    "Xtr_3 = Xtr_3.reshape(len(Xtr_3), -1)\n",
    "Xva_3 = Xva_3.reshape(len(Xva_3), -1)\n",
    "\n",
    "Xtr_3_std, Xva_3_std = standardize_train_val(Xtr_3, Xva_3)\n",
    "\n",
    "in_dim_1=Xtr_1_std.shape[1]\n",
    "in_dim_2=Xtr_2_std.shape[1]\n",
    "in_dim_3=Xtr_3_std.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "487972fb-78a2-418b-9ec9-78811de9878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Softmax Warmup] epoch 01 | train_acc=0.663, val_acc=0.667\n",
      "[Softmax Warmup] epoch 02 | train_acc=0.669, val_acc=0.678\n",
      "[Softmax Warmup] epoch 03 | train_acc=0.613, val_acc=0.609\n",
      "[Softmax Warmup] epoch 04 | train_acc=0.665, val_acc=0.667\n",
      "[Softmax Warmup] epoch 05 | train_acc=0.667, val_acc=0.674\n",
      "[Softmax Warmup] epoch 06 | train_acc=0.675, val_acc=0.681\n",
      "[Softmax Warmup] epoch 07 | train_acc=0.676, val_acc=0.686\n",
      "[Softmax Warmup] epoch 08 | train_acc=0.640, val_acc=0.636\n",
      "[Softmax Warmup] epoch 09 | train_acc=0.669, val_acc=0.673\n",
      "[Softmax Warmup] epoch 10 | train_acc=0.674, val_acc=0.672\n",
      "\n",
      "FashionMNIST0.3:\n",
      "\n",
      "Final train_acc: 0.6736\n",
      "Final val_acc: 0.6717\n"
     ]
    }
   ],
   "source": [
    "softmax_warm_1 = SoftmaxWarmup(in_dim_1, num_classes=3, lr=0.02, weight_decay=5e-4)\n",
    "softmax_warm_1.fit(Xtr_1_std, ytr_1, Xva_1_std, yva_1,\n",
    "                 epochs=10, batch_size=512, verbose=True, seed=0)\n",
    "\n",
    "train_acc_1 = softmax_warm_1.score(Xtr_1_std, ytr_1)\n",
    "val_acc_1 = softmax_warm_1.score(Xva_1_std, yva_1)\n",
    "print()\n",
    "print(\"FashionMNIST0.3:\")\n",
    "print(f\"\\nFinal train_acc: {train_acc_1:.4f}\")\n",
    "print(f\"Final val_acc: {val_acc_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80946042-b8ea-44e7-ab69-c4c83bf6fcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Softmax Warmup] epoch 01 | train_acc=0.377, val_acc=0.356\n",
      "[Softmax Warmup] epoch 02 | train_acc=0.338, val_acc=0.340\n",
      "[Softmax Warmup] epoch 03 | train_acc=0.347, val_acc=0.321\n",
      "[Softmax Warmup] epoch 04 | train_acc=0.381, val_acc=0.363\n",
      "[Softmax Warmup] epoch 05 | train_acc=0.384, val_acc=0.351\n",
      "[Softmax Warmup] epoch 06 | train_acc=0.390, val_acc=0.353\n",
      "[Softmax Warmup] epoch 07 | train_acc=0.372, val_acc=0.337\n",
      "[Softmax Warmup] epoch 08 | train_acc=0.391, val_acc=0.367\n",
      "[Softmax Warmup] epoch 09 | train_acc=0.352, val_acc=0.344\n",
      "[Softmax Warmup] epoch 10 | train_acc=0.366, val_acc=0.336\n",
      "\n",
      "FashionMNIST0.6:\n",
      "\n",
      "Final train_acc: 0.3661\n",
      "Final val_acc: 0.3358\n"
     ]
    }
   ],
   "source": [
    "softmax_warm_2 = SoftmaxWarmup(in_dim_2, num_classes=3, lr=0.02, weight_decay=5e-4)\n",
    "softmax_warm_2.fit(Xtr_2_std, ytr_2, Xva_2_std, yva_2,\n",
    "                 epochs=10, batch_size=512, verbose=True, seed=0)\n",
    "\n",
    "train_acc_2 = softmax_warm_2.score(Xtr_2_std, ytr_2)\n",
    "val_acc_2 = softmax_warm_2.score(Xva_2_std, yva_2)\n",
    "print()\n",
    "print(\"FashionMNIST0.6:\")\n",
    "print(f\"\\nFinal train_acc: {train_acc_2:.4f}\")\n",
    "print(f\"Final val_acc: {val_acc_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1b04ad-870e-4f08-8c33-41f3f426c174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Softmax Warmup] epoch 01 | train_acc=0.350, val_acc=0.332\n",
      "[Softmax Warmup] epoch 02 | train_acc=0.351, val_acc=0.335\n",
      "[Softmax Warmup] epoch 03 | train_acc=0.358, val_acc=0.341\n",
      "[Softmax Warmup] epoch 04 | train_acc=0.364, val_acc=0.340\n",
      "[Softmax Warmup] epoch 05 | train_acc=0.368, val_acc=0.347\n",
      "[Softmax Warmup] epoch 06 | train_acc=0.366, val_acc=0.346\n",
      "[Softmax Warmup] epoch 07 | train_acc=0.366, val_acc=0.347\n",
      "[Softmax Warmup] epoch 08 | train_acc=0.371, val_acc=0.342\n",
      "[Softmax Warmup] epoch 09 | train_acc=0.365, val_acc=0.337\n",
      "[Softmax Warmup] epoch 10 | train_acc=0.367, val_acc=0.335\n",
      "\n",
      "CIFAR:\n",
      "\n",
      "Final train_acc: 0.3675\n",
      "Final val_acc: 0.3347\n"
     ]
    }
   ],
   "source": [
    "softmax_warm_3 = SoftmaxWarmup(in_dim_3, num_classes=3, lr=0.02, weight_decay=5e-4)\n",
    "softmax_warm_3.fit(Xtr_3_std, ytr_3, Xva_3_std, yva_3,\n",
    "                 epochs=10, batch_size=512, verbose=True, seed=0)\n",
    "\n",
    "train_acc_3 = softmax_warm_3.score(Xtr_3_std, ytr_3)\n",
    "val_acc_3 = softmax_warm_3.score(Xva_3_std, yva_3)\n",
    "print()\n",
    "print(\"CIFAR:\")\n",
    "print(f\"\\nFinal train_acc: {train_acc_3:.4f}\")\n",
    "print(f\"Final val_acc: {val_acc_3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "086d935a-5cb8-488c-a746-a42756fce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_warm_1 = CNNWarmup(image_shape=(28,28), channels=1, num_classes=3,\n",
    "#                      lr=1e-3, weight_decay=5e-4, batch_size=256, epochs=10, seed=0)\n",
    "# cnn_warm_1.fit(Xtr_1, ytr_1, Xva_1, yva_1, verbose=True)\n",
    "# print(\"Final train_acc:\", cnn_warm_1.score(Xtr_1, ytr_1))\n",
    "# print(\"Final val_acc:\",   cnn_warm_1.score(Xva_1, yva_1))\n",
    "\n",
    "\n",
    "# cnn_warm_cifar = CNNWarmup(image_shape=(32,32), channels=3, num_classes=3,\n",
    "#                            lr=1e-3, weight_decay=5e-4, batch_size=256, epochs=15, seed=0)\n",
    "# cnn_warm_cifar.fit(Xtr3, ytr3, Xva3, yva3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8b1ed-bdea-4867-bfca-2e38b5737ae2",
   "metadata": {},
   "source": [
    "# 4 估计前2个数据集的T，比对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10f5cccc-6eee-47ec-b913-0d71ab700b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_T(T_hat: np.ndarray, T_true: np.ndarray, verbose: bool = True):\n",
    "    \n",
    "    assert T_hat.shape == T_true.shape, \"T_hat.shape != T_true\"\n",
    "    diff = T_hat - T_true\n",
    "\n",
    "    # (1) 每行 L1 差\n",
    "    per_row_L1 = np.sum(np.abs(diff), axis=1)\n",
    "\n",
    "    # (2) Frobenius 范数\n",
    "    frob = np.linalg.norm(diff, 'fro')\n",
    "\n",
    "    # (3) 最大绝对误差\n",
    "    max_abs = np.max(np.abs(diff))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"T_hat:\\n\", np.round(T_hat, 4))\n",
    "        print(\"T_true:\\n\", np.round(T_true, 4))\n",
    "        print(\"-\" * 30)\n",
    "        print(\"per row L1:\", np.round(per_row_L1, 4))\n",
    "        print(\"Frobenius:\", round(float(frob), 4))\n",
    "        print(\"max|Δ|:\", round(float(max_abs), 4))\n",
    "\n",
    "    return {\n",
    "        \"per_row_L1\": per_row_L1,\n",
    "        \"frobenius\": frob,\n",
    "        \"max_abs\": max_abs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb06d22f-59c9-43d8-8c2f-d0a6de0d56f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==== 先把两套真 T 写成常量（来自作业说明） ====\n",
    "T_true_03 = np.array([[0.7, 0.3, 0.0],\n",
    "                      [0.0, 0.7, 0.3],\n",
    "                      [0.3, 0.0, 0.7]], dtype=np.float32)\n",
    "\n",
    "T_true_06 = np.array([[0.4, 0.3, 0.3],\n",
    "                      [0.3, 0.4, 0.3],\n",
    "                      [0.3, 0.3, 0.4]], dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4c76eec-eea5-4e2b-b2e2-f112b9b2c83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!(  FashionMNIST0.3  )!!\n",
      "\n",
      "---------Softmax+Counts α=0.05--------------- \n",
      "T_hat:\n",
      " [[0.7306 0.2153 0.0542]\n",
      " [0.0042 0.7361 0.2597]\n",
      " [0.2444 0.0042 0.7514]]\n",
      "T_true:\n",
      " [[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "------------------------------\n",
      "per row L1: [0.1694 0.0806 0.1111]\n",
      "Frobenius: 0.1405\n",
      "max|Δ|: 0.0847\n",
      "\n",
      "---------Softmax+Counts α=0.1--------------- \n",
      "T_hat:\n",
      " [[0.7306 0.2271 0.0424]\n",
      " [0.0042 0.7125 0.2833]\n",
      " [0.266  0.0028 0.7312]]\n",
      "T_true:\n",
      " [[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "------------------------------\n",
      "per row L1: [0.1458 0.0333 0.0681]\n",
      "Frobenius: 0.1031\n",
      "max|Δ|: 0.0729\n",
      "\n",
      "---------Softmax+Counts α=0.2--------------- \n",
      "T_hat:\n",
      " [[0.7104 0.2483 0.0413]\n",
      " [0.0062 0.7139 0.2799]\n",
      " [0.2774 0.0038 0.7188]]\n",
      "T_true:\n",
      " [[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "------------------------------\n",
      "per row L1: [0.1035 0.0403 0.0451]\n",
      "Frobenius: 0.0775\n",
      "max|Δ|: 0.0517\n",
      "\n",
      "---------Softmax+Counts α=0.22--------------- \n",
      "T_hat:\n",
      " [[0.708  0.2503 0.0417]\n",
      " [0.0063 0.708  0.2857]\n",
      " [0.2828 0.0066 0.7105]]\n",
      "T_true:\n",
      " [[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "------------------------------\n",
      "per row L1: [0.0994 0.0287 0.0343]\n",
      "Frobenius: 0.0709\n",
      "max|Δ|: 0.0497\n",
      "\n",
      "---------Softmax+Counts α=0.25--------------- \n",
      "T_hat:\n",
      " [[0.7022 0.2561 0.0417]\n",
      " [0.0083 0.7039 0.2878]\n",
      " [0.2869 0.0125 0.7006]]\n",
      "T_true:\n",
      " [[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "------------------------------\n",
      "per row L1: [0.0878 0.0244 0.0261]\n",
      "Frobenius: 0.065\n",
      "max|Δ|: 0.0439\n",
      "\n",
      "---------Softmax+Counts α=0.28--------------- \n",
      "T_hat:\n",
      " [[0.6953 0.2604 0.0444]\n",
      " [0.0112 0.699  0.2899]\n",
      " [0.2854 0.0231 0.6915]]\n",
      "T_true:\n",
      " [[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "------------------------------\n",
      "per row L1: [0.0888 0.0223 0.0461]\n",
      "Frobenius: 0.0679\n",
      "max|Δ|: 0.0444\n",
      "\n",
      "---------Softmax+Counts α=0.3--------------- \n",
      "T_hat:\n",
      " [[0.6912 0.259  0.0498]\n",
      " [0.0134 0.6935 0.2931]\n",
      " [0.284  0.0363 0.6796]]\n",
      "T_true:\n",
      " [[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "------------------------------\n",
      "per row L1: [0.0995 0.0269 0.0727]\n",
      "Frobenius: 0.0806\n",
      "max|Δ|: 0.0498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"!!(  FashionMNIST0.3  )!!\\n\")\n",
    "P_1 = softmax_warm_1.predict_proba(Xtr_1_std)  # (N, C)\n",
    "\n",
    "for a in [0.05, 0.1, 0.2, 0.22, 0.25, 0.28, 0.3]:\n",
    "    T_hat_1, _ = estimate_T_from_probs(P_1, ytr_1, top_pct=a, use_counts=True, verbose=False)\n",
    "    print(f\"---------Softmax+Counts α={a}--------------- \")\n",
    "    #print(f\"Softmax+Counts α={a}: \\n\", np.round(T_hat,4))\n",
    "    evaluate_T(T_hat_1, T_true_03)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e26178a3-322b-48af-aeda-fb26e3159fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!(  FashionMNIST0.6  )!!\n",
      "\n",
      "---------Softmax+Counts α=0.05--------------- \n",
      "T_hat:\n",
      " [[0.4819 0.2667 0.2514]\n",
      " [0.2694 0.4444 0.2861]\n",
      " [0.2722 0.3569 0.3708]]\n",
      "T_true:\n",
      " [[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n",
      "------------------------------\n",
      "per row L1: [0.1639 0.0889 0.1139]\n",
      "Frobenius: 0.1347\n",
      "max|Δ|: 0.0819\n",
      "\n",
      "---------Softmax+Counts α=0.08--------------- \n",
      "T_hat:\n",
      " [[0.4531 0.283  0.2639]\n",
      " [0.2865 0.4297 0.2839]\n",
      " [0.2856 0.3628 0.3516]]\n",
      "T_true:\n",
      " [[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n",
      "------------------------------\n",
      "per row L1: [0.1063 0.0594 0.1257]\n",
      "Frobenius: 0.1107\n",
      "max|Δ|: 0.0628\n",
      "\n",
      "---------Softmax+Counts α=0.1--------------- \n",
      "T_hat:\n",
      " [[0.4403 0.2917 0.2681]\n",
      " [0.2993 0.416  0.2847]\n",
      " [0.2743 0.3653 0.3604]]\n",
      "T_true:\n",
      " [[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n",
      "------------------------------\n",
      "per row L1: [0.0806 0.0319 0.1306]\n",
      "Frobenius: 0.0984\n",
      "max|Δ|: 0.0653\n",
      "\n",
      "---------Softmax+Counts α=0.15--------------- \n",
      "T_hat:\n",
      " [[0.4116 0.2991 0.2894]\n",
      " [0.306  0.4069 0.287 ]\n",
      " [0.2815 0.3704 0.3481]]\n",
      "T_true:\n",
      " [[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n",
      "------------------------------\n",
      "per row L1: [0.0231 0.0259 0.1407]\n",
      "Frobenius: 0.0921\n",
      "max|Δ|: 0.0704\n",
      "\n",
      "---------Softmax+Counts α=0.2--------------- \n",
      "T_hat:\n",
      " [[0.3944 0.2997 0.3059]\n",
      " [0.3156 0.3955 0.2889]\n",
      " [0.2833 0.376  0.3406]]\n",
      "T_true:\n",
      " [[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n",
      "------------------------------\n",
      "per row L1: [0.0118 0.0312 0.1521]\n",
      "Frobenius: 0.1002\n",
      "max|Δ|: 0.076\n",
      "\n",
      "---------Softmax+Counts α=0.25--------------- \n",
      "T_hat:\n",
      " [[0.3947 0.2975 0.3078]\n",
      " [0.3153 0.3886 0.2961]\n",
      " [0.2864 0.3686 0.345 ]]\n",
      "T_true:\n",
      " [[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n",
      "------------------------------\n",
      "per row L1: [0.0156 0.0306 0.1372]\n",
      "Frobenius: 0.0916\n",
      "max|Δ|: 0.0686\n",
      "\n",
      "---------Softmax+Counts α=0.3--------------- \n",
      "T_hat:\n",
      " [[0.3859 0.2979 0.3162]\n",
      " [0.3194 0.3863 0.2942]\n",
      " [0.2894 0.3704 0.3403]]\n",
      "T_true:\n",
      " [[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n",
      "------------------------------\n",
      "per row L1: [0.0324 0.0389 0.1407]\n",
      "Frobenius: 0.0985\n",
      "max|Δ|: 0.0704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"!!(  FashionMNIST0.6  )!!\\n\")\n",
    "P_2 = softmax_warm_2.predict_proba(Xtr_2_std)  # (N, C)\n",
    "\n",
    "for a in [0.05, 0.08, 0.1, 0.15, 0.2, 0.25, 0.3]:\n",
    "    T_hat_2, _ = estimate_T_from_probs(P_2, ytr_2, top_pct=a, use_counts=True, verbose=False)\n",
    "    print(f\"---------Softmax+Counts α={a}--------------- \")\n",
    "    #print(f\"Softmax+Counts α={a}: \\n\", np.round(T_hat,4))\n",
    "    evaluate_T(T_hat_2, T_true_06)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353cdeb-cefb-48cb-b74f-7cf87239b609",
   "metadata": {},
   "source": [
    "# 5 估计CIFAR的T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75dce095-e4f0-4ed9-afc0-a228c8fb0747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!(  CIFAR  )!!\n",
      "\n",
      "Softmax+Counts α=0.05: \n",
      " [[0.415  0.3317 0.2533]\n",
      " [0.3033 0.375  0.3217]\n",
      " [0.26   0.3033 0.4367]]\n",
      "\n",
      "Softmax+Counts α=0.08: \n",
      " [[0.4188 0.3302 0.251 ]\n",
      " [0.2948 0.3604 0.3448]\n",
      " [0.2802 0.299  0.4208]]\n",
      "\n",
      "Softmax+Counts α=0.1: \n",
      " [[0.4008 0.3392 0.26  ]\n",
      " [0.2942 0.3775 0.3283]\n",
      " [0.2833 0.3017 0.415 ]]\n",
      "\n",
      "Softmax+Counts α=0.12: \n",
      " [[0.4014 0.3368 0.2618]\n",
      " [0.2986 0.3729 0.3285]\n",
      " [0.2847 0.2972 0.4181]]\n",
      "\n",
      "Softmax+Counts α=0.15: \n",
      " [[0.405  0.335  0.26  ]\n",
      " [0.2956 0.3706 0.3339]\n",
      " [0.29   0.2956 0.4144]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"!!(  CIFAR  )!!\\n\")\n",
    "P_3 = softmax_warm_3.predict_proba(Xtr_3_std)  # (N, C)\n",
    "#参考fashionmnist0.6的情况\n",
    "for a in [0.05, 0.08, 0.10, 0.12, 0.15]:\n",
    "    T_hat_3, _ = estimate_T_from_probs(P_3, ytr_3, top_pct=a, use_counts=True, verbose=False)\n",
    "    #print(f\"---------Softmax+Counts α={a}--------------- \")\n",
    "    print(f\"Softmax+Counts α={a}: \\n\", np.round(T_hat_3,4))\n",
    "    #evaluate_T(T_hat_2, T_true_06)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6ee3b-dfc0-41b7-bf09-df166340d2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
