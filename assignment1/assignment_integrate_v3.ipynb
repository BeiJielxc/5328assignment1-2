{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP5328 - Advanced Machine Learning\n",
    "## Assignment 1: Non-negative Matrix Factorization\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Semester 2, 2025)**\n",
    "\n",
    "In this ipython notebook, we provide some example code for assignment1.\n",
    "+ Load Data.\n",
    "    - ORL dataset. \n",
    "    - Extended YaleB dataset. \n",
    "    - AR dataset (**optional**).\n",
    "+ Perform Evaluation. \n",
    "   - Relative Reconstruction Errors.\n",
    "   - Accuracy, NMI (**optional**).\n",
    "\n",
    "Lecturer: Tongliang Liu.\n",
    "\n",
    "**Note: All datasets can be used only for this assignment and you are not allowed to distribute these datasets. If you want to use AR dataset, you need to apply it by yourself (we do not provide AR dataset due to the problem of license, please find more details in http://www2.ece.ohio-state.edu/~aleix/ARdatabase.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "### 1.0 Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"   # 避免 MKL+Windows 这个 known issue 的内存警告\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"KMeans is known to have a memory leak on Windows with MKL\",\n",
    "    category=UserWarning\n",
    ")\n",
    "#set file index\n",
    "p =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxrwxrwx@ 41 carmenfung  staff  1312  8 25  2018 \u001b[30m\u001b[43mCroppedYaleB\u001b[m\u001b[m\n",
      "drwx------@ 44 carmenfung  staff  1408  8 24  2018 \u001b[34mORL\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# The structure of data folder.\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tree structure of data folder.\n",
    "├── CroppedAR\n",
    "    ├── M-001-01.bmp\n",
    "    ├── M-001-01.txt\n",
    "    ├── M-001-02.bmp\n",
    "    ├── M-001-02.txt\n",
    "    ├── ...\n",
    "├── CroppedYaleB\n",
    "│   ├── yaleB01\n",
    "│   ├── yaleB02\n",
    "│   ...\n",
    "│   ├── yaleB38\n",
    "│   └── yaleB39\n",
    "└── ORL\n",
    "    ├── s1\n",
    "    ├── s2\n",
    "    ├── s3\n",
    "    ├── ...\n",
    "    ├── s40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load ORL Dataset and Extended YaleB Dataset.\n",
    "+ ORL dataset contains ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). The size of each image is 92x112 pixels, with 256 grey levels per pixel. To further reduce the computation complexity, you can resize all images to 30x37 pixels.\n",
    "\n",
    "+ Extended YaleB dataset contains 2414 images of 38 human subjects under 9 poses and 64 illumination conditions. All images are manually aligned, cropped, and then resized to 168x192 pixels. To further reduce the computation complexity, you can resize all images to 42x48 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "#record the path to reload the origin data\n",
    "\n",
    "def load_data(root='data/CroppedYaleB'):\n",
    "    \"\"\" \n",
    "    Load ORL (or Extended YaleB) dataset to numpy array.\n",
    "    \n",
    "    Args:\n",
    "        root: path to dataset.\n",
    "        reduce: scale factor for zooming out images.\n",
    "        \n",
    "    \"\"\" \n",
    "    images, labels,paths = [], [],[]\n",
    "\n",
    "    for i, person in enumerate(sorted(os.listdir(root))):\n",
    "        \n",
    "        if not os.path.isdir(os.path.join(root, person)):\n",
    "            continue\n",
    "        \n",
    "        for fname in os.listdir(os.path.join(root, person)):    \n",
    "            \n",
    "            # Remove background images in Extended YaleB dataset.\n",
    "            if fname.endswith('Ambient.pgm'):\n",
    "                continue\n",
    "            \n",
    "            if not fname.endswith('.pgm'):\n",
    "                continue\n",
    "                \n",
    "            # load image.\n",
    "            img = Image.open(os.path.join(root, person, fname))\n",
    "            #new -record the path\n",
    "            img_path = os.path.join(root, person, fname)\n",
    "            img = img.convert('L') # grey image.\n",
    "\n",
    "            # TODO: preprocessing.\n",
    "            # --- c fixed size (keeps PIL (W,H) order) ---\n",
    "            root_name = os.path.basename(os.path.normpath(root)).lower()\n",
    "\n",
    "            # --- normalize to [0,1] (nonnegative, NMF-safe) ---\n",
    "            arr = np.asarray(img, dtype=np.float64) / 255.0\n",
    "            # convert image to numpy column vector (after preprocessing).\n",
    "            img = arr.reshape((-1, 1))\n",
    "\n",
    "            #no-normalize\n",
    "            img = np.asarray(img).reshape((-1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # collect data and label.\n",
    "            images.append(img)\n",
    "            labels.append(i)\n",
    "            #collect the path\n",
    "            paths.append(img_path)\n",
    "\n",
    "    # concate all images and labels.\n",
    "    images = np.concatenate(images, axis=1)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 胡椒函数 +对应的噪音图展示 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#看起来正确的胡椒算法\n",
    "def add_salt_pepper_noise(V, p= None, r=None, rng=None):\n",
    "    V_noisy = V.copy()\n",
    "    d, n = V.shape\n",
    "    total_pixels = d * n\n",
    "    num_corrupt = int(total_pixels * p)\n",
    "\n",
    "\n",
    "    idx = rng.choice(total_pixels, size=num_corrupt, replace=False)\n",
    "    num_white = int(num_corrupt * r)\n",
    "    white_idx = idx[:num_white]\n",
    "    black_idx = idx[num_white:]\n",
    "\n",
    "\n",
    "    V_noisy.flat[white_idx] = 1.0\n",
    "    V_noisy.flat[black_idx] = 0.0\n",
    "    return V,V_noisy\n",
    "\n",
    "# Plot result. \n",
    "def plot_noise_pic(V,img_size,ind = None, ps = None,rs = None):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1, total, 1)\n",
    "    plt.imshow(V_hat[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "    plt.title('Image(Original)')\n",
    "    plt.subplot(1, total, 2)\n",
    "    plt.imshow(V_noise[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "    plt.title('Noise')\n",
    "    i = 3\n",
    "    for p in ps:\n",
    "        for r in rs:\n",
    "            V_hat ,V  = add_salt_pepper_noise(V=V_hat,p=p,r=r )\n",
    "            # Plot result. \n",
    "            plt.subplot(1, total, i)\n",
    "            plt.imshow(V[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray,vmin= 0,vmax=1)\n",
    "            plt.title(\"p={}, r={}\".format(p, r))\n",
    "            i+=1\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NMF代码实现 + NMF之后的可视化展示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算两个目标值之间的相对改进量\n",
    "def relative_improvement(prev_obj, curr_obj, eps=1e-12):\n",
    "    return abs(prev_obj - curr_obj) / (abs(prev_obj) + eps)\n",
    "\n",
    "#定义统一的可视化函数\n",
    "def visualize_reconstruction(V_hat, V_noisy, V_recon, ind=50, img_size=(92,112), title=\"NMF Reconstruction\"):\n",
    "    plt.suptitle(title, size=16)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(V_hat[:, ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "    plt.title(\"Image (Original)\")\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(V_noisy[:, ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "    plt.title(\"Image (Noisy)\")\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(V_recon[:, ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "    plt.title(\"Image (Reconstructed)\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "eps = 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 L1-NMF代码实现+实现后的效果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_nmf(V, H, W, steps=3000, tol=1e-5, verbose=False):\n",
    "\n",
    "    obj_prev = np.sum(np.abs(V - W @ H))\n",
    "    for step in range(1, steps + 1):\n",
    "        # 残差与权重（IRLS）：Q_ij = 1/(|E_ij| + eps)\n",
    "        E  = V - (W @ H)\n",
    "        # Q  = 1.0 / (np.abs(E) + eps)\n",
    "        Q = 1/np.maximum(np.abs(E), eps) #new\n",
    "        Q = np.minimum(Q, 1e6)  #new\n",
    "        \n",
    "\n",
    "        # 乘法更新（带逐元素权重的加权-L2 形式）\n",
    "        # H <- H .* (W^T (Q ⊙ V)) / (W^T (Q ⊙ (W H)))\n",
    "        H *= (W.T @ (Q * V)) / (W.T @ (Q * (W @ H)) + eps)\n",
    "        H = np.maximum(H, 0.0) #new\n",
    "\n",
    "        # W <- W .* ((Q ⊙ V) H^T) / ((Q ⊙ (W H)) H^T)\n",
    "        WH = W @ H\n",
    "        W *= ((Q * V) @ H.T) / (((Q * WH) @ H.T) + eps)\n",
    "        W = np.maximum(W, 0.0) #new\n",
    "\n",
    "        # 目标与收敛\n",
    "        obj = np.sum(np.abs(V - W @ H))\n",
    "        rel = relative_improvement(obj_prev, obj, eps=eps)\n",
    "        # if verbose and (step % 100 == 0):\n",
    "        #     print(f\"[L1-NMF] step={step}, obj={obj:.6e}, rel={rel:.3e}\")\n",
    "        if rel < tol:\n",
    "            if verbose:\n",
    "                print(f\"[L1-NMF] Converged at step={step}, obj={obj:.6e}, rel={rel:.3e}\")\n",
    "            break\n",
    "        obj_prev = obj\n",
    "\n",
    "    return  W,H, step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 L2-NMF代码实现+实现后的效果可视化\n",
    "\n",
    "\n",
    "For any data matrix $X$, L2-NMF finds non-negative factors $D \\in \\mathbb{R}^{w \\times k}_{+}$ and $R \\in \\mathbb{R}^{k \\times n}_{+}$ such that\n",
    "$$\n",
    "X \\approx DR,\n",
    "$$\n",
    "by minimizing the Frobenius reconstruction error\n",
    "$$\n",
    "\\min_{D,R \\ge 0}\\ \\lVert X - DR \\rVert_F^2 .\n",
    "$$\n",
    "\n",
    "Using multiplicative update rules (MUR), the iteration can be written as\n",
    "$$\n",
    "R \\leftarrow R \\odot \\frac{D^\\top X}{D^\\top D R}, \\qquad\n",
    "D \\leftarrow D \\odot \\frac{X R^\\top}{D R R^\\top},\n",
    "$$\n",
    "where $\\odot$ denotes element-wise multiplication.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_NMF(V,H,W,steps,tol=1e-5,verbose=False):\n",
    "    # H is coeffient matrix\n",
    "    # W is dictionary matrix\n",
    "    # V is the matrix with noise\n",
    "    #V hat  is the original matrix\n",
    "    # print(\"basic_NMF\")\n",
    "    obj_prev = np.linalg.norm(V - W @ H, 'fro')**2\n",
    "    \n",
    "    for step in range(1, steps+1):    \n",
    "\n",
    "        H_star = H * (W.T.dot(V)) / (W.T.dot(W).dot(H) + eps)\n",
    "        W_star = W * (V.dot(H_star.T)) / (W.dot(H_star.dot(H_star.T)) + eps)\n",
    "\n",
    "        H, W = H_star, W_star\n",
    "\n",
    "        obj = np.linalg.norm(V - W @ H, 'fro')**2\n",
    "\n",
    "        rel_impr = relative_improvement(obj_prev, obj, eps=eps)\n",
    "\n",
    "        if rel_impr < tol:\n",
    "            if verbose:\n",
    "                print(f\"Converged at step={step}, obj={obj:.6e}, rel_impr={rel_impr:.3e}\")\n",
    "            break\n",
    "\n",
    "        obj_prev = obj\n",
    "\n",
    "        if verbose and step in (1000, 5000, 10000, 15000, 20000):\n",
    "            print(f\"step={step}, obj={obj:.6e}, rel_impr={rel_impr:.3e}\")\n",
    "\n",
    "    return W,H, step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 L2,1-NMF代码实现+实现后的效果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L21_nmf(V, H, W, steps=3000, tol=1e-5, verbose=False):\n",
    "\n",
    "    def L21_obj(E):\n",
    "        return np.sum(np.sqrt(np.sum(E * E, axis=1) + eps))\n",
    "\n",
    "    obj_prev = L21_obj(V - W @ H)\n",
    "\n",
    "    for step in range(1, steps + 1):\n",
    "        # 残差 & 行权重\n",
    "        E = V - (W @ H)                               # (d, n)\n",
    "        row_norm = np.sqrt(np.sum(E * E, axis=1) + eps)  # (d,)\n",
    "        s = 1.0 / (row_norm + eps)                    # (d,)\n",
    "\n",
    "        # 广播实现 S = diag(s)\n",
    "        SV  = s[:, None] * V\n",
    "        SWH = s[:, None] * (W @ H)\n",
    "\n",
    "        # H <- H .* (W^T (S V)) / (W^T (S W H))\n",
    "        H *= (W.T @ SV) / (W.T @ SWH + eps)\n",
    "\n",
    "        # W <- W .* ((S V) H^T) / ((S W H) H^T)\n",
    "        SWH = s[:, None] * (W @ H)                    # 用更新后的 H 重新计算\n",
    "        W  *= (SV @ H.T) / (SWH @ H.T + eps)\n",
    "\n",
    "        # 收敛判定（相对改进）\n",
    "        obj = L21_obj(V - W @ H)\n",
    "        rel = relative_improvement(obj_prev, obj, eps=eps)\n",
    "        # if verbose and (step % 100 == 0):\n",
    "        #     print(f\"[L2,1-NMF] step={step}, obj={obj:.6e}, rel={rel:.3e}\")\n",
    "        if rel < tol:\n",
    "            if verbose:\n",
    "                print(f\"[L2,1-NMF] Converged at step={step}, obj={obj:.6e}, rel={rel:.3e}\")\n",
    "            break\n",
    "        obj_prev = obj\n",
    "\n",
    "    return W, H, step\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 CIM-NMF代码实现+实现后的效果可视化\n",
    "\n",
    "For any data matrix $X$, CIM-NMF finds non-negative factors \n",
    "$D \\in \\mathbb{R}_+^{d \\times k}$ and \n",
    "$R \\in \\mathbb{R}_+^{k \\times n}$ such that\n",
    "\n",
    "$$\n",
    "X \\approx DR,\n",
    "$$\n",
    "\n",
    "by minimizing the Correntropy-Induced Metric (CIM) reconstruction error:\n",
    "\n",
    "$$\n",
    "\\min_{D,R \\ge 0} \\sum_{i=1}^d \\sum_{j=1}^n \n",
    "\\left( 1 - \\frac{1}{\\sqrt{2\\pi}\\delta} \n",
    "\\exp\\!\\left(-\\frac{(X_{ij} - (DR)_{ij})^2}{2\\delta^2}\\right) \\right).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Multiplicative Update Rules (MUR)\n",
    "\n",
    "Using the weight matrix\n",
    "\n",
    "$$\n",
    "W_{ij} = \\frac{1}{\\sqrt{2\\pi}\\delta} \n",
    "\\exp\\!\\left(-\\frac{(X_{ij} - (DR)_{ij})^2}{2\\delta^2}\\right),\n",
    "$$\n",
    "\n",
    "the iteration can be written as\n",
    "\n",
    "$$\n",
    "R \\leftarrow R \\odot \\frac{D^\\top (W \\odot X)}{D^\\top (W \\odot (DR))}, \n",
    "\\qquad\n",
    "D \\leftarrow D \\odot \\frac{(W \\odot X) R^\\top}{(W \\odot (DR)) R^\\top},\n",
    "$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cim_nmf(V, H,W, max_iter=300, tol=1e-5, rng=None, verbose=False):\n",
    "    \"\"\"\n",
    "    CIM-NMF (Correntropy-Induced Metric NMF)\n",
    "    min_{W,H >=0} sum_{ij} r((V-WH)_{ij}, delta)\n",
    "\n",
    "    更新规则基于权重加权的 NMF:\n",
    "      H <- H * (W^T (R ⊙ V)) / (W^T (R ⊙ WH))\n",
    "      W <- W * ((R ⊙ V) H^T) / ((R ⊙ WH) H^T)\n",
    "    其中 R = exp(-(E^2)/(2 delta^2)) / (sqrt(2π) delta)\n",
    "    \"\"\"\n",
    "\n",
    "    # m, n = V.shape\n",
    "    # rng = np.random.default_rng(rng)\n",
    "    # W = np.maximum(rng.random((m, k)), 1e-8)\n",
    "    # H = np.maximum(rng.random((k, n)), 1e-8)\n",
    "\n",
    "    obj_prev = np.inf\n",
    "    eps = 1e-12\n",
    "    delta = 1\n",
    "\n",
    "    for it in range(1, max_iter + 1):\n",
    "        # restucture\n",
    "        V_hat = W @ H\n",
    "        E = V - V_hat\n",
    "\n",
    "        R = np.exp(-(E**2) / (2 * delta**2)) / (np.sqrt(2*np.pi) * delta)\n",
    "\n",
    "        obj = np.sum(1 - R)\n",
    "\n",
    "        # update H\n",
    "        H *= (W.T @ (R * V)) / (W.T @ (R * V_hat) + eps)\n",
    "        # update W\n",
    "        V_hat = W @ H\n",
    "        W *= ((R * V) @ H.T) / ((R * V_hat) @ H.T + eps)\n",
    "\n",
    "        rel_impr = relative_improvement(obj_prev, obj, eps)\n",
    "        if rel_impr < tol:\n",
    "            if verbose:\n",
    "                print(f\"[CIM-NMF] Converged at iter={it}, obj={obj:.6e}, rel_impr={rel_impr:.3e}\")\n",
    "            break\n",
    "        obj_prev = obj\n",
    "\n",
    "        # if verbose and it % 50 == 0:\n",
    "        #     print(f\"[CIM-NMF] iter={it}, obj={obj:.6e}, rel_impr={rel_impr:.3e}\")\n",
    "\n",
    "    return W, H, it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics\n",
    "\n",
    "\n",
    "### 4.1 Relative Reconstruction Errors (RRE)\n",
    "\n",
    "To compare the robustness of different NMF algorithms, you can use the ```relative reconstruction errors```. Let $V$ denote the contaminated dataset (by adding noise), and $\\hat{V}$\n",
    " denote the clean dataset. Let $W$ and $H$ denote the factorization results on $V$, the ``relative reconstruction errors`` then can be defined as follows:\n",
    " \\begin{equation}\n",
    "    RRE = \\frac{ \\| \\hat{V} - WH \\|_F }{ \\| \\hat{V} \\|_F}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Evaluate relative reconstruction errors.\n",
    "# 每个算法跑5次\n",
    "def RRE_calcu(V_hat,V_star,algo,iter_time):\n",
    "    # print(f\"==> Evaluate RRE for {algo} for the {iter_time} time \")\n",
    "    RRE = np.linalg.norm(V_hat - V_star) / np.linalg.norm(V_hat)\n",
    "    # print('RRE = {}'.format(RRE))\n",
    "    return RRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate Clustering Performance\n",
    "\n",
    "1. Accuracy.\n",
    "    \n",
    "    $$ Acc(Y, Y_{pred}) = \\frac{1}{n}\\sum\\limits_{i=1}^n 1\\{Y_{pred}(i) == Y(i)\\}$$\n",
    "        \n",
    "2. Normalized Mutual Information (NMI).\n",
    "\n",
    "    $$ NMI(Y, Y_{pred}) = \\frac{2 * I(Y, Y_{pred})}{H(Y) + H(Y_{pred})} $$\n",
    "    \n",
    "   where $ I(\\cdot,\\cdot) $ is mutual information and $ H(\\cdot) $ is entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other evaluate -检测指标\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "def assign_cluster_label(X, Y):\n",
    "    kmeans = KMeans(n_clusters=len(set(Y))).fit(X)\n",
    "    Y_pred = np.zeros(Y.shape)\n",
    "    for i in set(kmeans.labels_):\n",
    "        ind = kmeans.labels_ == i\n",
    "        Y_pred[ind] = Counter(Y[ind]).most_common(1)[0][0] # assign label.\n",
    "    return Y_pred\n",
    "\n",
    "\n",
    "def accu_calcu(H,Y_hat,algo,iter_time):\n",
    "    # print(f\"==> Evaluate Acc and NMI for {algo} for the {iter_time} time \")\n",
    "    # Assign cluster labels.\n",
    "    Y_pred = assign_cluster_label(H.T, Y_hat)\n",
    "    \n",
    "    acc = accuracy_score(Y_hat, Y_pred)\n",
    "    nmi = normalized_mutual_info_score(Y_hat, Y_pred)\n",
    "    # print('Acc(NMI) = {:.4f} ({:.4f})'.format(acc, nmi))\n",
    "    return acc, nmi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 执行鲁棒实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF algo_choice\n",
    "# ===== L1 =====\n",
    "def nmf_L1(V, K, local_seed, steps=3000, tol=1e-5, verbose=False):\n",
    "    rng = np.random.RandomState(local_seed)\n",
    "    N  = V.shape[1]   \n",
    "    M  = V.shape[0]\n",
    "    # init W, H\n",
    "    W = np.maximum(rng.rand(M, K), 1e-8)\n",
    "    H = np.maximum(rng.rand(K, N), 1e-8)\n",
    "    W_star, H_star,step_star = L1_nmf(V, H, W, steps=steps, tol=tol, verbose=verbose)\n",
    "    return W_star, H_star,step_star\n",
    "\n",
    "# ===== L2=====\n",
    "def nmf_L2(V, K, local_seed, steps=3000, tol=1e-5, verbose=False):\n",
    "    rng = np.random.RandomState(local_seed)\n",
    "    N  = V.shape[1]   \n",
    "    M  = V.shape[0]\n",
    "    # init W, H\n",
    "    W = rng.rand(M, K)\n",
    "    H = rng.rand(K, N)\n",
    "    W_star, H_star,step_star = L2_NMF(V, H, W, steps=steps, tol=tol, verbose=verbose)\n",
    "    return W_star, H_star,step_star\n",
    "\n",
    "# ===== L2,1 =====\n",
    "def nmf_L21(V, K, local_seed, steps=3000, tol=1e-5, verbose=True):\n",
    "    rng = np.random.RandomState(local_seed)\n",
    "    N  = V.shape[1]   \n",
    "    M  = V.shape[0]\n",
    "    # init W, H\n",
    "    W = rng.rand(M, K)\n",
    "    H = rng.rand(K, N)\n",
    "    W_star, H_star,step_star = L21_nmf(V, H, W, steps=steps, tol=tol, verbose=True)\n",
    "    return W_star, H_star,step_star\n",
    "\n",
    "# ===== CIM =====\n",
    "def nmf_CIM(V, K, local_seed, step=3000, tol=1e-5, verbose=True, **kwargs):\n",
    "    rng = np.random.RandomState(local_seed)\n",
    "    N  = V.shape[1]   \n",
    "    M  = V.shape[0]\n",
    "    # init W, H\n",
    "    W = rng.rand(M, K)\n",
    "    H = rng.rand(K, N)\n",
    "    W_star, H_star,step_star = cim_nmf(V, H,W, max_iter=step, tol=tol, rng=rng, verbose=True, **kwargs)\n",
    "    return W_star, H_star,step_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_time_nmf(V_hat_all,Y_hat_all,algo,K,p,r,img_size,iter_time,local_seed):\n",
    "    rng = np.random.RandomState(local_seed)\n",
    "    \n",
    "    #  90% subset\n",
    "    n = V_hat_all.shape[1]\n",
    "    m = int(np.ceil(0.9 * n))\n",
    "    idx = rng.choice(n, size=m, replace=False)   \n",
    "    V_sub = V_hat_all[:, idx]\n",
    "    y_sub = Y_hat_all[idx]\n",
    "    \n",
    "    #add noise\n",
    "    V_hat,V_noisy = add_salt_pepper_noise(V_sub, p=p, r=r, rng=rng)\n",
    "\n",
    "    #algo\n",
    "    W_star, H_star,step_star = algo(V_hat,K,local_seed)\n",
    "    V_star = W_star @ H_star\n",
    "    print(f\"{algo} final run step:\",step_star)\n",
    "    # visualize_reconstruction(V_hat, V_noisy, V_star, 50, img_size, title=f\"{algo} Reconstruction\")\n",
    "\n",
    "    #metrics-不确定该部分.应该用\n",
    "    rre = RRE_calcu(V_hat,V_star,algo,iter_time)\n",
    "    acc, nmi = accu_calcu(H_star,y_sub,algo,iter_time)\n",
    "    \n",
    "    return rre, acc, nmi \n",
    "    \n",
    "def robost_pipeline(dataset, algos, noise_list,n_runs = 1,global_seed = 42):\n",
    "    #load data\n",
    "    V_hat, Y_hat = load_data(root=f\"data/{dataset}\")   \n",
    "    print(f\"{dataset} dataset: X.shape = {V_hat.shape}, Y.shape = {Y_hat.shape}\")\n",
    "\n",
    "    if dataset == \"ORL\":\n",
    "        img_size = (92,112)\n",
    "        K = 40\n",
    "    elif dataset == \"CroppedYaleB\":\n",
    "        img_size = (168,192)\n",
    "        K= 38\n",
    "    else: \n",
    "        print(\"can not find the dataset\")\n",
    "\n",
    "    #wolk through all the algo\n",
    "    for algo_index, algo in enumerate(algos):\n",
    "        #add_noise\n",
    "        for noise_index,noise in enumerate(noise_list):\n",
    "            p, r = noise\n",
    "            rre_results = []\n",
    "            acc_results = []\n",
    "            nmi_results = []\n",
    "            for i in range(n_runs):\n",
    "                run_index = i\n",
    "                local_seed = (algo_index+1)*100 + (noise_index+1)*10+run_index\n",
    "                print(f\"algo is {algo},noise is {noise}, iter_time is {run_index}, the local_seed is {local_seed}\")\n",
    "\n",
    "                rre, acc, nmi  = one_time_nmf(V_hat,Y_hat,algo,K,p,r,img_size,run_index,local_seed)\n",
    "                rre_results.append(rre)\n",
    "                acc_results.append(acc)\n",
    "                nmi_results.append(nmi)\n",
    "            mean_rre, std_rre = np.mean(rre_results), np.std(rre_results, ddof=1)\n",
    "            mean_acc, std_acc = np.mean(acc_results), np.std(acc_results, ddof=1)\n",
    "            mean_nmi, std_nmi = np.mean(nmi_results), np.std(nmi_results, ddof=1)\n",
    "\n",
    "            print(f\"[Algo={algo.__name__}, Noise={noise}, Run={run_index}] \"\n",
    "                  f\"RRE={mean_rre:.4f}±{np.std(std_rre):.4f}, \"\n",
    "                  f\"ACC={mean_acc:.4f}±{np.std(std_acc):.4f}, \"\n",
    "                  f\"NMI={mean_nmi:.4f}±{np.std(std_nmi):.4f}\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORL dataset: X.shape = (10304, 400), Y.shape = (400,)\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.2, 0.1), iter_time is 0, the local_seed is 110\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 136\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.2, 0.1), iter_time is 1, the local_seed is 111\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 155\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.2, 0.1), iter_time is 2, the local_seed is 112\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 155\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.2, 0.1), iter_time is 3, the local_seed is 113\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 133\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.2, 0.1), iter_time is 4, the local_seed is 114\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 143\n",
      "[Algo=nmf_L1, Noise=(0.2, 0.1), Run=4] RRE=0.3056±0.0000, ACC=0.1961±0.0000, NMI=0.3770±0.0000\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.2, 0.7), iter_time is 0, the local_seed is 120\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 167\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.2, 0.7), iter_time is 1, the local_seed is 121\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 141\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.2, 0.7), iter_time is 2, the local_seed is 122\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 169\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.2, 0.7), iter_time is 3, the local_seed is 123\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 135\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.2, 0.7), iter_time is 4, the local_seed is 124\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 156\n",
      "[Algo=nmf_L1, Noise=(0.2, 0.7), Run=4] RRE=0.3062±0.0000, ACC=0.2000±0.0000, NMI=0.3806±0.0000\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.4, 0.1), iter_time is 0, the local_seed is 130\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 140\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.4, 0.1), iter_time is 1, the local_seed is 131\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 157\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.4, 0.1), iter_time is 2, the local_seed is 132\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 146\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.4, 0.1), iter_time is 3, the local_seed is 133\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 127\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.4, 0.1), iter_time is 4, the local_seed is 134\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 127\n",
      "[Algo=nmf_L1, Noise=(0.4, 0.1), Run=4] RRE=0.3064±0.0000, ACC=0.1917±0.0000, NMI=0.3825±0.0000\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.4, 0.7), iter_time is 0, the local_seed is 140\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 135\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.4, 0.7), iter_time is 1, the local_seed is 141\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 136\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.4, 0.7), iter_time is 2, the local_seed is 142\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 157\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.4, 0.7), iter_time is 3, the local_seed is 143\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 143\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.4, 0.7), iter_time is 4, the local_seed is 144\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 134\n",
      "[Algo=nmf_L1, Noise=(0.4, 0.7), Run=4] RRE=0.3067±0.0000, ACC=0.2011±0.0000, NMI=0.3819±0.0000\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.6, 0.1), iter_time is 0, the local_seed is 150\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 154\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.6, 0.1), iter_time is 1, the local_seed is 151\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 132\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.6, 0.1), iter_time is 2, the local_seed is 152\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 144\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.6, 0.1), iter_time is 3, the local_seed is 153\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 135\n",
      "algo is <function nmf_L1 at 0x1288ac0e0>,noise is (0.6, 0.1), iter_time is 4, the local_seed is 154\n",
      "<function nmf_L1 at 0x1288ac0e0> final run step: 151\n",
      "[Algo=nmf_L1, Noise=(0.6, 0.1), Run=4] RRE=0.3064±0.0000, ACC=0.2011±0.0000, NMI=0.3852±0.0000\n",
      "algo is <function nmf_L2 at 0x1288ac220>,noise is (0.2, 0.1), iter_time is 0, the local_seed is 210\n",
      "<function nmf_L2 at 0x1288ac220> final run step: 1776\n",
      "algo is <function nmf_L2 at 0x1288ac220>,noise is (0.2, 0.1), iter_time is 1, the local_seed is 211\n",
      "<function nmf_L2 at 0x1288ac220> final run step: 1856\n",
      "algo is <function nmf_L2 at 0x1288ac220>,noise is (0.2, 0.1), iter_time is 2, the local_seed is 212\n",
      "<function nmf_L2 at 0x1288ac220> final run step: 1843\n",
      "algo is <function nmf_L2 at 0x1288ac220>,noise is (0.2, 0.1), iter_time is 3, the local_seed is 213\n"
     ]
    }
   ],
   "source": [
    "dataset_orl = 'ORL'\n",
    "algos = [nmf_L1,nmf_L2,nmf_L21,nmf_CIM]\n",
    "noise_list = [(0.2,0.1),(0.2,0.7),(0.4,0.1),(0.4,0.7),(0.6,0.1)]\n",
    "r = 5\n",
    "\n",
    "df_result = robost_pipeline(dataset_orl, algos, noise_list,n_runs = r,global_seed = 42)\n",
    "dataset_yale = 'CroppedYaleB'\n",
    "df_result_yale = robost_pipeline(dataset_yale, algos, noise_list,n_runs = r,global_seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise visualization\n",
    "plot_noise_pic(V,img_size,ind = 50, ps = ps,rs = rs)\n",
    "# # NMF -visualization\n",
    "# visualize_reconstruction(V_hat, V_noisy, V_star, 50, img_size, title=f\"{algo} Reconstruction\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
